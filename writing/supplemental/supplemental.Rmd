---
title: "\\LARGE Parents scaffold the formation of conversational pacts with their children (SOM-R)"
# author: "\\large \\emph{Ashley, Leung, Alexandra Tunkel, and Daniel Yurovsky}"
author: "\\large \\emph{XX, XX, and XX}"
header-includes:
  - \usepackage[section]{placeins}
  - \usepackage{float}
  - \floatplacement{figure}{h!} # make every figure with caption = t
  - \raggedbottom
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
documentclass: article
bibliography: tangramgame-si.bib
fontsize: 11pt
geometry: margin=1in
csl: apa6.csl
---

```{r load-libraries, message=FALSE, warning=FALSE, include = F}
library(here)
library(knitr)
library(papaja)
library(kableExtra)
library(tidyverse)
library(tidyboot)
library(janitor)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
library(effectsize)
library(glue)
library(ggthemes)

opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE, 
               tidy = FALSE, echo = FALSE)

theme_set(theme_few(base_size = 12))

options(digits=3)
```

\renewcommand\thesection{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}

\section{Norming Experiment}

```{r anonymize-norming-data, eval = FALSE}
# anonymize raw csv
# read_csv(here('data/norming/dataFromMongo.csv')) %>%
#   filter(iterationName %in% c('full_sample', 'full_sample2')) %>%
#   filter(trial_type == 'survey-text') %>%
#   mutate(wID = as.numeric(as.factor(wID))) %>%
#   write_csv(here('./data/norming/anonymizedDataFromMongo.csv'))

# Our current hypothesis is that we're not seeing big context effects for adults in the pilot because even the 'close' contexts aren't close along the actual representational dimensions people are using to label things. 
# So if their labeling prior in isolation is to say it's 'sad', that's still highly informative in the close context because the distractor doesn't look sad at all and there's no pressure being put on the prior 
# (kids likely have very different priors, e.g. 'guy' or 'person' with high probability, so these same contexts probably would put pressure on them). 
# To do this in adults, we need to elicit priors in isolation for more tangrams (e.g. all 40) and then design better 'close' contexts where we put together tangrams where adults have very similar priors. 
```

```{r load-norming-data, eval = FALSE}
# take the first 31 rows of each wID. 
# gets the first game of people who did it twice -- Mongo returns objectIDs in order of insertion date
d.raw.labels <- read_csv(here("data/norming/anonymizedDataFromMongo.csv")) 

initialAssignmentIDs <- d.raw.labels %>%
  group_by(wID) %>%
  summarize(firstAssignmentID = first(aID)) %>%
  pull(firstAssignmentID)

complete_games.norming <- d.raw.labels %>%
  filter(aID %in% initialAssignmentIDs) %>%
  group_by(wID) %>%
  tally() %>%
  filter(n == 31) %>%
  pull(wID)

passed_catch <- d.raw.labels %>%
  filter(target_type == 'catch') %>%
  mutate(utterance = tolower(utterance)) %>%
  filter(utterance == 'strawberry') %>%
  pull(wID)

d <- d.raw.labels %>%
  filter(wID %in% complete_games.norming) %>%
  filter(wID %in% passed_catch) %>%
  filter(target_type != 'catch') %>%
  mutate(clean_utt = tolower(utterance),
         lemmatized_utt = lemmatize_words(clean_utt),
         lemmatized_utt = str_replace_all(lemmatized_utt, regex("\\W+"), ""))
```

```{r eval = FALSE}
# How unbalanced is our sample across targets?
d %>%
  group_by(target) %>%
  tally() %>%
  ggplot(aes(x = n)) +
    geom_histogram(bins = 5)
```

```{r eval = FALSE}
d %>%
  group_by(wID) %>%
  arrange(trial_index) %>%
  mutate(rep = (lemmatized_utt == lag(lemmatized_utt, 1) &
                lemmatized_utt == lag(lemmatized_utt, 2))) %>%
  filter(rep) %>%
  arrange(wID) 
  # group_by(wID) %>%
  # summarize(meanRT = mean(rt)) %>%
  filter(rt < 2000)
  ggplot(aes(x = meanRT)) +
    geom_histogram(bins = 20)

```


Find most similar sets by JS/KL divergence

```{r get-pair-kl, eval = FALSE}
get_pair_data <- function(df, tangram1, tangram2) {
  df %>%
    filter(target == tangram1) %>%
    select(lemmatized_utt, n) %>%
    rename(target_n = n) %>%
    left_join(df %>%
                filter(target == tangram2) %>%
                select(lemmatized_utt, n),
              by = "lemmatized_utt") %>%
    rename(comparison_n = n)
}

# TODO: take all pairwise comparisons
completed_data <- d %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) 

pairs <- d %>% 
  distinct(target) %>%
  pull() %>%
  combn(2) %>%
  t() %>%
  as_tibble(.name_repair = "unique") %>%
  rename(tangram1 = `...1`, tangram2 = `...2`) %>%
  mutate(target_id = tangram1,
         comparison_id = tangram2) %>%
  group_by(target_id, comparison_id) %>%
  nest() 
```

```{r, eval = FALSE}
pairs %>%
  arrange(kl) 


```

```{r scatterplots, eval = FALSE}
#Look at codeability, etc., as a function of our norming data 
norming_overlap <- read_csv(here("data/norming/jsPsychStims.csv")) %>%
  select(target, overlap_diff) %>%
  right_join(kls, by = "target") %>%
  filter(target_type == "tangram")

ggplot(norming_overlap, aes(x = overlap_diff, y = kl)) +
  geom_point() +
  facet_wrap(~comparison)
```

```{r overall overlap, eval = FALSE}
# highest and lowest overlap pairs?
dissimilar <- overall %>%
  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
  arrange(overlap) %>%
  filter(overlap < median(overlap)) %>%
  filter(!duplicated(target_id)) %>%
  mutate(dissimilar = comparison_id,
         dvalue = overlap) %>%
  select(-c(comparison_id, overlap, n))

similar <- overall %>%
  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(overlap > median(overlap)) %>%
  arrange(desc(overlap)) %>%
  filter(!duplicated(target_id)) %>%
  mutate(similar = comparison_id,
         svalue = overlap) %>%
  select(-c(comparison_id, overlap, n))

#TODO: Tangrams that have high and low similarity matches

matches <- left_join(dissimilar, similar, by = "target_id") %>%
  arrange(dvalue, desc(svalue)) %>%
  write.csv(here("data/norming/high_low_matches.csv"))
  
#matches <- overall %>%
#  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
#  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
#  filter(target_id %in% c(dissimilar$target_id, dissimilar$comparison_id) |
#        comparison_id %in% c(dissimilar$target_id, dissimilar$comparison_id)) %>%
#  arrange(desc(overlap))

# function to find targets with high and low similarity matches

#df = NULL

#match_pairs <- function(data, comparison){
#  for (i in 1:data.length) {
#    ifelse (data$target_id %in% c(comparison$target_id, comparison$comparison_id) |
#        comparison_id %in% c(comparison$target_id, comparison$comparison_id), 
#        bind_rows(data[i,], df), return)
#  }
#}
```


To construct the "close" and "far" conditions, we required an estimate of the extent to which different tangrams would elicit similar names.
We recruited $XXX$ adult participants to provide one-word names for $YYY$ tangrams, each presented in isolation.
This distribution of names was intended to reflect the  "prior" over valid labels that adults might bring into a reference game. 
We then generated close (and far) pairs to maximize (and minimize) naming similarity using the following algorithm.

First, we computed the naming similarity between all pairs of stimuli, using the same metric reported in the main text.
For each target tangram $t$, we then found the unique distractors $d_{close} = \arg \min_{t'}s(t, t')$ and $d_{far} = \arg \max_{t'} s(t, t')$ and computed the range $T(t) = |s(t, d_{far}) - s(t, d_{close})|$. 
We then picked the single target with largest range and added it to our stimulus set, along with its close and far distractors.
We then removed all three tangrams $\{t, d_{close}, d_{far}\}$ from the overall set, and repeated this procedure until we obtained 10 unique, non-overlapping sets to use in our production study.

\section{Additional comprehensibility analyses}

\subsection{Response times are faster for parent descriptions}

Because accuracy was near ceiling, we also considered the measure of response time.
We find that descriptions produced by parents lead to faster guesses, but there is still no effect of experiment phase.

```{r full-rt-lmer}
database_ids <- read_csv(here("data/experiment2/databaseid.csv"), show_col_types = FALSE) %>%
  rename(gameid = subid) %>%
  select(gameid, age)

raw_adult_data <- read_csv(here("data/experiment2/comprehension_adults.csv"),
                           show_col_types = FALSE) %>%
  select(-age) %>%
  filter(!audioid %in% c("67_08", "67_08_b"))

check_correct <- raw_adult_data %>%
  filter(occurrence == "check") %>%
  mutate(check_correct = correct) %>%
  select(subid, check_correct)

passed_check <- check_correct %>%
  group_by(check_correct) %>%
  count() %>%
  pivot_wider(names_from = check_correct, values_from = n) %>%
  clean_names()

tidy_turk_data <- raw_adult_data %>%
  left_join(check_correct, by = c("subid")) %>%
  filter(check_correct, occurrence != "check") %>%
  separate(audioid, into = c("gameid", "gametrial"), sep = "_", remove = F) %>%
  mutate(gameid = as.numeric(gameid),
         gametrial = as.numeric(gametrial)) %>%
  left_join(database_ids, by = ("gameid")) %>%
  mutate(log_rt = log(reactiontime)) %>%
  pivot_longer(cols = c(correct, reactiontime, log_rt), names_to = "measure") %>%
  mutate(comprehender_group = 'adult',
         occurrence = as.numeric(occurrence))
```

```{r reduced-rt-lmer}
rt_lmer <- tidy_turk_data %>%
  filter(measure == "log_rt") %>%
  lmer(value ~ person + age + occurrence  + 
                   (1|target) + (1|subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)

full_rt_lmer <- tidy_turk_data %>%
  filter(measure == "log_rt") %>%
  lmer(value ~ person * age * occurrence + 
                   (1|target) + (1|subid), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)
```

\subsection{Comprehensibility decreases with exchange length}

Finally, we consider whether descriptions produced in the context of a long back-and-forth exchange tend to be less comprehensible (see Appendix). 
We analyze this relationship that we didn't have room for in the main text.

```{r exchange-data}
exchanges_data <- read_csv(here("data/experiment1/exchanges.csv"),
                           show_col_types = FALSE) 

d.exchanges.raw <- exchanges_data %>%
  mutate(age = as.character(age),
         age = if_else(is.na(age), 'adult', age),
         director = case_when(person == "parent" & role == "matcher" ~ "child",
                              person == "child" & role == "director" ~ "child",
                              T ~ "adult")) %>%
  group_by(director, person, subid, trial, age, target, rep_num) %>%
  summarize(num_exchanges = n()) %>% 
  ungroup()

turk_exchange_data <- d.exchanges.raw %>% 
  group_by(trial, subid) %>% 
  summarise(num_exchanges = sum(num_exchanges)) %>% 
  right_join(tidy_turk_data, by = c("subid" = "gameid"), "trial" = "gametrial")

turk_exchange_data %>%
  filter(measure == "correct") %>%
  group_by(age, num_exchanges, person) %>%
  summarise(value = mean(value)) %>%
  ggplot(aes(x = num_exchanges, y = value, color = as.factor(age))) +
  facet_wrap(~ person) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

\section{Other measures of variability and overlap}

```{r comparing-distributions, eval = FALSE}
# kl_drop <- function(df, x1, x2) {
#   x1 <- enquo(x1)
#   x2 <- enquo(x2)
#   # 
#   df %>%
#     filter(!!x1 != 0 | !!x2 != 0) %>%
#     summarise(kl = KL.shrink(!!x1, !!x2)) %>%
#     pull(kl)
# }
# 
# kls <- d.prod %>%
#   count(target_type, competitor_type, target, clean_utt) %>%
#   complete(nesting(target_type, target), competitor_type, clean_utt, 
#            fill = list(n = 0)) %>%
#   pivot_wider(names_from = competitor_type, values_from = n) %>%
#   group_by(target_type, target) %>%
#   nest() %>%
#   mutate(close_far = map(data, ~kl_drop(.x,close, far)),
#          isolated_far = map(data, ~kl_drop(.x,isolated, far)),
#          isolated_close = map(data, ~kl_drop(.x, close, isolated))) %>%
#   unnest(cols = c(close_far, isolated_far, isolated_close)) %>%
#   select(-data) %>%
#   pivot_longer(cols = c(close_far, isolated_far, isolated_close), 
#                names_to = "comparison", values_to = "kl")
# 
# 
# ggplot(kls, aes(x = comparison, y = kl)) + 
#   facet_wrap(~ target_type) + 
#   geom_boxplot()
```


\newpage

\section*{References}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\noindent
<div id = "refs"></div>
\endgroup