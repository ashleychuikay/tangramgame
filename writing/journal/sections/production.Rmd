# Experiment 3: Production experiment

In Experiment 2, we found that naive children and adults were able to comprehend referential expressions equally well.
Contrary to the classical view that young children are rigidly fixating on a single way of conceptualizing the tangram, even young children seem remarkably flexible in their ability to accommodate different descriptions.
That is, to the extent that children contributed to poorer group performance in Experiment 1, their contribution is not well-explained by the *comprehension hypothesis*.
At the same time, however, we found some evidence consistent with the *production hypothesis*: messages originally produced by children were somewhat less comprehensible for listeners of any age (as indexed by slower reaction times and lower accuracy).

While this source effect is intriguing, it is difficult to disentangle the original messages from the interactive parent-child context in which they were produced. 
For example, it is possible that children were receiving interactive scaffolding that prompted them toward more comprehensible expressions. 
Or, conversely, it is possible that children were relying on their parents to take on more of the division of labor of interpretation [@hawkins2021division] and were thus producing expressions that are *less* comprehensible to naive audience than they were actually capable of.
In Experiment 3, we remove the interactive context to more directly assess children's ability to produce referential expressions for novel tangram objects in different contexts. 
We used a 2x2 design aiming to tease apart two related explanations for poor production performance.
First, to assess the extent to which production difficulties stem from *pragmatic reasoning* (i.e. the ability to recognize that an accessible label is not sufficiently informative in context), we manipulate the whether the distractor in context is more or less similar to the target. 
Second, to assess the extent to which production difficulties stem from impoverished *lexical priors* (i.e. the ability to access candidate labels for a given referent), we manipulate whether the target objects are familiar photographs or novel tangram shapes. 
To the extent that children fail to produce context-sensitive utterances even for familiar objects with accessible labels, we may expect that pragmatic reasoning is the primary bottleneck on performance.
To the extent to which these difficulties are limited to novel objects, we may expect that lexical priors play a larger role.

## Methods

```{r e3-anonymize-data, eval = FALSE}
# this block pre-processes the raw, un-anonymized data from adults
# we keep it here for reproducibility but only release the preprocessed data
# read_csv(here("data/experiment3/dataFromMongo.csv"))%>%
#   filter(trial_type == "survey-text") %>%
#   filter(iterationName == "pilot2") %>%
#   mutate(wID = as.numeric(as.factor(wID))) %>%
#   write_csv(here('./data/experiment3/anonymizedDataFromMongo.csv'))
```

```{r e3-read-adult-data}
NUM_TRIALS <- 48
d.prod.adult <- here("data/experiment3/anonymizedDataFromMongo.csv") %>%
  read_csv(show_col_types = FALSE) %>%
  mutate(age_group = 'adult')
  
complete_games.adult <- d.prod.adult %>%
  group_by(wID) %>%
  tally() %>%
  filter(n >= NUM_TRIALS) %>%
  pull(wID)
```

```{r e3-combine-data, eval = FALSE}
# this block pre-processes the raw, un-anonymized data from kids
# we keep it here for reproducibility but only release the preprocessed data for privacy

# d.prod.kid <- list.files(here("data/experiment3/kid/"), "*.csv",
#                           full.names = TRUE) %>%
#   map_dfr(read_csv, show_col_types = FALSE) %>%
#   mutate(age_group = 'kid') %>%
#   rename(wID = id) %>%
#   select(-trial_type, -internal_node_id, -trialtype, -test_part, -responses)
# 
# d.prod <- d.prod.adult %>%
#   filter(wID %in% complete_games.adult) %>%
#   group_by(wID) %>%
#   slice(1:NUM_TRIALS) %>%
#   filter(rt > 1000) %>%
#   bind_rows(d.prod.kid) %>%
#   unite(id, age_group, wID, remove = F) %>%
#   mutate(utterance = str_to_lower(utterance),
#          utterance = str_trim(utterance),
#          utt_length = str_length(utterance),
#          clean_utt = lemmatize_strings(utterance),
#          clean_utt = str_remove_all(clean_utt, pattern = ' '))
# 
# write_csv(d.prod, here("data/experiment3/production_data_lemmatized.csv"))
```

```{r e3-read-lemmatized_data}
# d.prod.clean <- read_csv(
#   here("data/experiment3/production_data_lemmatized.csv"),
#   guess_max = 5000,
#   show_col_types = FALSE)%>%
#   mutate(button_pressed = as.numeric(button_pressed))

# manually cleaned data
d.prod.clean.manual <- read_csv(
  here("data/experiment3/production_data_lemmatized_cleaned.csv"),
  guess_max = 5000,
  show_col_types = FALSE)%>%
  mutate(button_pressed = as.numeric(button_pressed))

```

```{r e3-stim-details}
n_stims <- d.prod.clean.manual %>%
  distinct(target_type, competitor_type, target, foil) 

fam_stims <- n_stims %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()

novel_stims <- n_stims  %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()
```

### Participants

We recruited `r length(complete_games.adult)` adult participants from Amazon Mechanical Turk. All participants gave informed consent prior to the start of the study and were compensated 60Â¢. 
We also recruited \textcolor{red}{XX children} to participate in the study. \textcolor{red}{DEMOGRAPHICS/ages}. The study was conducted online over Zoom.

### Stimuli and Design

We used `r length(fam_stims)` pictures of familiar objects and `r length(novel_stims)` pictures of tangrams. 
Familiar objects were drawn from the set used by @degen2020 using eight different basic-level categories that we expected to be familiar to children: bears, birds, cars, candy, dogs, fish, shirts, and tables.
Tangrams were drawn from a publicly royalty free set available on \textcolor{red}{XX tangrams site}.

```{r e3-design, out.height = "450px", fig.align = "center", set.cap.width=T, fig.cap = "Objects in Experiment 3 could appear in isolation (A), in a far context (B), or in a close context (C)."}
knitr::include_graphics(here("writing/journal/diagrams/e3_diagram.pdf"))
```

For both familiar objects and novel tangrams, trials were divided into three types: isolation trials in which a single target object was presented (Fig. \ref{fig:e3-design}A), far trials in which a target was paired with a competitor of the same type but with low semantic overlap (Fig. \ref{fig:e3-design}B), and close trials in which the target was paired with a competitor with high semantic overlap (Fig. \ref{fig:e3-design}C). For familiar objects, close trials involved two objects in the same basic-level semantic category (e.g. Pug and German Shepherd), and far trials involved two objects in different basic-level categories (e.g. Pug and Rabbit). For novel tangrams, close and far competitors were determined on the basis of their degree of labeling overlap with target in a norming experiment. In this norming experiment, we asked participants to label a number of tangrams in isolation. We used naming agreement--the proportion of responses that were the same for two tangrams--to estimate their semantic similarity [see @zettersten2020]. Details of the novel tangram selection procedure are described in the Supporting Information.

Adults provided labels for a total of eight familiar objects and eight novel tangrams. Each object appeared three times: once in isolation, once in a close context, and once in a far context. All trials of a single type were blocked (e.g. isolation familiar objects), and the six blocks were presented in random order across participants. 
Children provided labels for the same eight familiar and novel objects. To maintain children's attention for the duration of the experiment, they gave responses only in the close and far conditions and not in the isolation condition. Children's productions in all conditions were typed into a text box in real time during their participation.

Children also participated in an additional check condition designed to measure their understanding of ambiguity in comprehension. The check condition's purpose was to aid in interpretation of potentially ambiguous utterances that children were expected to provide in the Close condition (e.g. saying "bear" to distinguish between a brown bear and a polar bear). In this condition, children were told that they would hear a label produced by another child, and that they should indicate whether this label unambiguously identified the target object. Three of the trials were far trials in which an unambiguously incorrect label was provided to the child (e.g. "table" to distinguish between a pug and a rabbit), three of the trials were close trials in which an unambiguously correct label was provided (e.g. "polar bear" to distinguish between a polar bear and a grizzly bear), and three were close trials in which an ambiguous label was provided (e.g. "dog" to distinguish between a husky and a dalmatian). For the check condition, children responded verbally ("Yes" or "No") and an experimenter selected the corresponding button on the screen.

### Procedure

After giving informed consent, adults read a introduction screen informing them that they would be doing an experiment in which they saw one or pictures on the screen at a time. One of these objects would be in a blue box, and they should describe this object as best as they could by typing one or two words into a text box. At the start of each block, they would be told whether they would see one picture (isolation), or two pictures (close and far), and reminded that they should type in a description that would help another participant identify the target in the blue border. Adults then completed all blocks in random order.

After parents gave consent and children assented to participate, children were told that they would see pictures and be asked to describe them. Children then completed several warm-up trials to introduce them to the game. On the first trial, they saw a picture of a red apple. On the next trial, they saw a pear with a blue border around it and the same apple and were asked to describe the object in the blue box. Then, the experimenter indicated that they would close their eyes on the next trial so that they wouldn't see what children saw. On this trial, children saw a white mug with a blue border around it and a white plate. While the experimenter's eyes were closed, the child was encouraged to describe the object in the blue box so that the experimenter could identify it. Once children provided a description, the blue border disappeared, and the experimenter opened their eyes. The experimenter then identified the object based on children's descriptions, and revealed the blue box again to verify whether they selected the correct target. 

The same procedure was repeated for three more trials with the following image pairs: a red purple ball and a plant; a white rose and a red rose; a round wooden table and a rectangular wooden table. The latter two trials contained object pairs where both objects shared a basic level category label (i.e., "flower" and "table"). These two trials served to subtly indicate to children that referential ambiguity is possible. If children did not uniquely identify the target (e.g. just saying "flower"), the experimenter would indicate that the child's expression was ambiguous, and the experimenter would not select the correct target. While these trials provided feedback about ambiguity to the children, the experimenter did not suggest alternate descriptions or examples of unambiguous expressions. After these warm-up trials, children were told that they would continue to play the labeling game and that their responses would be shown to another person who did not know which one was in the blue border. In contrast to adults, children always began with a block of familiar objects in either the close or far condition. They always completed the manipulation check block last.

### Preprocessing 

We cleaned the text input using a uniform rubric across the combined data set.
First, we manually corrected typos and removed stop words (e.g. determiners like 'a', 'the').
Second, we lemmatized all entries to remove spurious differences between tenses and plurals of the same root form. \textcolor{red}{[rdh: should we also do some kind of regularization of word order? like make sure 'man is kneeling' and 'kneeling man' are mapped to the same thing? otherwise even if we remove 'is' and remove white space, we'll have 'kneelman' and 'mankneel' as different tokens.]}
Third, we removed spaces and collapsed multiple words together into a single token (e.g. 'German Shepherd' was tokenized to 'germanshepherd'). \textcolor{red}{[rdh: was there anything else we want to say about how the experimenter typed in the kid data? what was the rubric in case of long rambling utterances? did you say part of it back to the kid and ask them to confirm or something?]}

## Results


### Response times reflect familiarity and context

```{r e2-rts, eval=FALSE}
# overall RT distribution
d.prod.clean.manual %>%
  filter(competitor_type != "check") %>%
  mutate(logged_rt = log(rt)) %>%
  filter(rt < 30000) %>%
  gather(measure, value, logged_rt, rt) %>%
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(age_group ~ measure, scales='free')

# RT by target type
d.prod.clean.manual %>%
  filter(competitor_type != "check") %>%
  mutate(logged_rt = log(rt)) %>%
  ggplot(aes(x = logged_rt, fill = target_type)) +
    geom_density(alpha = .5) +
    facet_wrap(~age_group) +
    theme(legend.position = "right")

# model of logged RTs
rt_model <- d.prod.clean.manual %>%
  filter(competitor_type != "check") %>%
  mutate(logged_rt = log(rt)) %>%
  lmer(logged_rt ~ age_group*target_type + (1|id) + (1|target), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```
  
```{r compare-lengths, eval = FALSE}
booted_lengths <- d.prod.clean.manual %>%
  group_by(age_group, target_type, competitor_type, id) %>%
  summarise(length = mean(utt_length)) %>%
  group_by(age_group, competitor_type, target_type) %>%
  tidyboot_mean(length)

booted_lengths %>%
  filter(competitor_type != 'check') %>%
  ggplot(aes(x = target_type, y = empirical_stat, color = competitor_type)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.5)) + 
  facet_wrap(~ age_group) + 
  labs(y = 'utterance length (# characters)', x='') +
  theme(legend.position = "top")
```

```{r eval = FALSE}
d.prod.clean.manual %>%
  filter(competitor_type != 'check') %>%
  ggplot(aes(x = target_type, y = log(rt), color = competitor_type)) +
    geom_boxplot() +
    facet_wrap(~ age_group) +
    theme(legend.position = 'top')
```

```{r}
rt_model <- d.prod.clean.manual %>% 
  filter(age_group=="adult") %>%
  lm(log(rt) ~ target_type * competitor_type, 
     data = .) %>%
  tidy()
```

\textcolor{red}{[rdh: do we want to mention the utterance length effect? kids produced much longer utterances for tangrams, which seems like an effect of its own]}
We examined log response times in adults' productions. Children's response times were not analyzed because they responded verbally, and an experimenter entered their responses for them. As such, we do not have accurate measurements of children's actual response times. In the following analysis, we predict response times from target and competitor types and their interactions.

Because objects in close contexts are more similar to one another, we expect longer response times on these trials due to more effort in uniquely identifying the referent. In our sample, adults' response times were longer on close context trials than far and isolated context trials [`r rt_model$estimate[1]`]. Due to the novelty of tangram images, we also expect longer response times on novel trials than on familiar trials. Indeed, we find that adults's response times were longer when describing tangram images than familiar images. In our experiment, response times were recorded between the onset of the trial and the point at which participants finished typing their responses, so longer response times may reflect greater effort in generating a description, as well as longer descriptions (and thus longer typing time). However, because all participants were instructed to give brief (two-word) descriptions, it is unlikely that description length contributes to all of the variance in response time differences.

Next, we move to our two core hypotheses.
First, could children be failing to take into account the referential context when deciding what to say, leading to more ambiguous or underinformative referring expressions?
Second, could children simply have weaker expectations over possible acceptable labels for novel objects, making retrieval challenging?
These mechanisms are not mutually exclusive --- and, indeed are the two fundamental components of recent production models [e.g. @murthy2021shades, @hawkins2022partners]. 

### Adult descriptions are more sensitive to referential context

```{r context-sensitivity, out.height = "300px", fig.align = "center", set.cap.width=T, fig.cap = "\\label{fig:e3_contextsensitivity} Results for Exp.~3."}

# we can just look at the item-by-item / speaker-by-speaker level 
# where there's a context effect...
indiv.measures <- d.prod.clean.manual %>%
  filter(competitor_type != 'check') %>%
  group_by(age_group, id, target_type, competitor_type, target, clean_utt) %>%
  tally() %>%
  group_by(age_group, id, target) %>%
  pivot_wider(names_from = competitor_type, values_from = c('clean_utt')) %>%
  unnest(cols = c(close, far, isolated)) %>%
  filter(!is.na(close), !is.na(far)) %>%
  mutate(close_far.distance = helpers$normalized_lv(close, far),
         close_isolated.distance = helpers$normalized_lv(close, isolated),
         far_isolated.distance = helpers$normalized_lv(far, isolated),
         far_close_overlap = far == close,
         far_isolated_overlap = far == isolated,
         close_isolated_overlap = close == isolated) 

indiv.measures.clean <- d.prod.clean.manual %>%
  filter(competitor_type != 'check') %>%
  group_by(age_group, id, target_type, competitor_type, target, clean_utt) %>%
  tally() %>%
  group_by(age_group, id, target) %>%
  pivot_wider(names_from = competitor_type, values_from = c('clean_utt')) %>%
  unnest(cols = c(close, far, isolated)) %>%
  filter(!is.na(close), !is.na(far)) %>%
  mutate(close_far.distance = helpers$normalized_lv(close, far),
         close_isolated.distance = helpers$normalized_lv(close, isolated),
         far_isolated.distance = helpers$normalized_lv(far, isolated),
         far_close_overlap = far == close,
         far_isolated_overlap = far == isolated,
         close_isolated_overlap = close == isolated) 

#measure context sensitivity by exact matches across conditions

contextsensitivity.means <- indiv.measures %>%
  group_by(age_group, target_type) %>%
  tidyboot_mean(far_close_overlap, na.rm = T) 

contextsensitivity.means.manual <- indiv.measures.clean %>%
  group_by(age_group, target_type) %>%
  tidyboot_mean(far_close_overlap, na.rm = T)

dodge = position_dodge(0.9)

contextsensitivity.means %>%
  ggplot(aes(x = age_group, y = empirical_stat, fill = target_type)) +
    geom_bar(stat = 'identity', position = dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
    labs(title = 'Proportion of exact matches across conditions',
         y = '% overlap (exact matches)', x = '') +
    theme(aspect.ratio = 1, legend.position = "right")

contextsensitivity.means.manual %>%
  ggplot(aes(x = age_group, y = empirical_stat, fill = target_type)) +
    geom_bar(stat = 'identity', position = dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
    labs(title = 'Proportion of exact matches across conditions (cleaned data)',
         y = '% overlap (exact matches)', x = '') +
    theme(aspect.ratio = 1, legend.position = "right")

#measure context sensitivity using LV distance

contextsensitivity.distance <- indiv.measures %>%
  group_by(age_group, target_type) %>%
  tidyboot_mean(close_far.distance, na.rm = T)

contextsensitivity.distance.manual <- indiv.measures.clean %>%
  group_by(age_group, target_type) %>%
  tidyboot_mean(close_far.distance, na.rm = T)

contextsensitivity.distance %>%
  ggplot(aes(x = age_group, y = empirical_stat, fill = target_type)) +
    geom_bar(stat = 'identity', position = dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
    labs(title = 'Levensthein distance between conditions', 
         y = 'Levenshtein   distance between close and far conditions', x = '') +
    theme(aspect.ratio = 1, legend.position = "right")

contextsensitivity.distance.manual %>%
  ggplot(aes(x = age_group, y = empirical_stat, fill = target_type)) +
    geom_bar(stat = 'identity', position = dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
    labs(title = 'Levensthein distance between conditions (cleaned data)',
         y = 'Levenshtein distance between close and far conditions', x = '') +
    theme(aspect.ratio = 1, legend.position = "right")
```

```{r contextsensitivitylmer}
indiv.measures.lmer <- indiv.measures %>%
  glmer(far_close_overlap ~ age_group * target_type + (1 | target) + (1 + target_type | id),
        family = 'binomial',
        data = .) %>%
  tidy() %>%
  filter(effect == 'fixed') %>%
  select(-group, -effect)

indiv.measures.softmeasure <- indiv.measures %>%
  lmer(close_far.distance ~ age_group * target_type + (1 | target) + (1 + target_type | id),
        data = .) %>%
  tidy() %>%
  filter(effect == 'fixed') %>%
  select(-group, -effect)

indiv.measures.inter <- indiv.measures.lmer %>% filter(term == 'age_groupkid:target_typetangram')
soft.inter <- indiv.measures.softmeasure %>% filter(term == 'age_groupkid:target_typetangram')

```

To test our first hypothesis, we examine extent to which the same participant produces different utterances across the *far* vs. *close* contexts.
We begin by considering a simple 'exact match' criterion, coding an item as 1 if the participant used the same label for that item in both contexts and 0 if they used different labels. 
This criterion is relatively conservative in the sense that it will miss a number of near- or partial-matches, so it can be viewed as a lower-bound on similarity. 
We predict overlap using a mixed-effects logistic regression model.
We include fixed effects for age cohort (child vs. adult) and target condition (familiar vs. novel objects), as well as their interaction.
We include random intercepts for both participants and items, and a random slope of target condition at the participant level, which was the most complex structure that converged.
We found a significant interaction, $b=`r indiv.measures.inter$estimate`, z = `r indiv.measures.inter$statistic`, p < 0.001$ (see Fig.~\ref{fig:e3_contextsensitivity}). 

Although adults displayed similar rates of context-sensitivity for familiar and novel objects (familiar: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'adult', target_type == 'familiar'))$empirical_stat`, novel: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'adult', target_type == 'tangram'))$empirical_stat`), children were nearly twice as likely to provide the exact same label across contexts for a familiar object (familiar: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'kid', target_type == 'tangram'))$empirical_stat`; novel: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'kid', target_type == 'familiar'))$empirical_stat`).
In other words, children showed some degree internal variance in how they referred to novel tangrams but likely produced the single most salient, accessible label for familiar objects.
Similar results were obtained using 'softer' measures like edit distance (the number of edits required to turn one string into the other), $b=`r soft.inter$estimate`, z = `r soft.inter$statistic`, p = `r soft.inter$p.value`$. 

### Familiar objects elicit less variable names

```{r eval = FALSE}
var.measures <- d.prod %>%
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type, target) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) / mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n)) %>%
  gather(measure, value, unique_labels:codeability)

var.measures %>%
  group_by(target_type, competitor_type, measure) %>%
  tidyboot_mean(value) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat = 'identity', position=dodge) +
    facet_grid(measure ~. , scales='free') +
    theme(legend.position = "top")

ggsave('variability_measures_clean.pdf', width = 10, height = 10, unit = 'in')

```


```{r regression-model-comparing-competitor-types, eval = FALSE}
var.measures %>%
  lmer(codeability ~ target_type*competitor_type + (1|target), data =.) %>%
  tidy() %>%
  filter(effect == "fixed")
```

```{r overlap-helpers, eval = FALSE}
# ps <- d.prod.clean %>%
#   distinct(wID)
# 
# first_half <- d.prod.clean %>%
#   filter(wID %in% (slice(ps, 1:(n()/2)) %>% pull(wID))) %>%
#   count(target, clean_utt) %>%
#   complete(target, clean_utt, 
#            fill = list(n = 0)) %>%
#   get_average_overlap()
# 
# second_half <- d %>%
#   filter(wID %in% (slice(ps, ((n()/2)+1):(n())) %>% pull(wID))) %>%
#   count(target, lemmatized_utt) %>%
#   complete(target, lemmatized_utt, 
#            fill = list(n = 0)) %>%
#   get_average_overlap()
# 
# # correlation b/w split data
# first_half %>%
#   rename(first_half = overlap) %>%
#   left_join(second_half, by = c("target_id", "comparison_id", "n")) %>%
#   summarise(cor = cor(first_half, overlap, method = "spearman")) %>%
#   summarise(cor = mean(cor, na.rm = T))
# 
# overall <- d %>%
#   #filter(!(target %in% c("R1.jpg", "O1.jpg"))) %>%
#   #mutate(target = as.factor(target)) %>%
#   count(target, lemmatized_utt) %>%
#   complete(target, lemmatized_utt, 
#            fill = list(n = 0)) %>%
#   get_average_overlap()
```

```{r plot-overlap, eval = FALSE} 
# ggplot(symmetric_overlap, aes(x = target_id, y = comparison_id, fill = log1p(overlap))) + 
#   geom_tile() +
  # theme(legend.position = 'right')
```

```{r pilot-items, eval = FALSE}
# pilot_items <- d.prod.clean %>% 
#   filter(target_type == "tangram", competitor_type != "isolated") %>%
#   distinct(target, foil, competitor_type) %>%
#   rename(target_id = target, comparison_id = foil) %>%
#   left_join(symmetric_overlap, by = c("target_id", "comparison_id"))
# 
# pilot_items %>%
#   group_by(competitor_type) %>%
#   tidyboot_mean(overlap)
```

```{r, eval = FALSE}
# pilot_items %>%
#   ggplot(aes(x = competitor_type, y = overlap)) +
#     geom_jitter(width = .1, height = .1) +
#     geom_boxplot(alpha = .5)
```

We test this hypothesis by comparing the 'spread' of the distribution of labels given to the same object across familiar and novel object classes. 

We considered several measures of codability/overlap:

* total number of unique labels (when people overlap a lot, this is smaller because we collect the same number of labels for each target; warning, this could be misleading if we're dropping trials due to high rts and some conditions have more dropped)
* proportion of unique labels to total labels (fixes this problem, but still doesn't account for the distribution of overlap, e.g. if one gets hit 10 times and others get hit 1 vs. everything gets hit 2 times.)
* entropy (we all know and love)
* codeability (what proportion of people provided the most common label, if there were 40 labels and 20 were 'bird', this would be 50%)

### Attention checks

\textcolor{red}{[rdh: where should we report these checks? in some sense it's a simple sanity check so we could lead with it? but it's also only relevant for the developmental sample, which suggests we should put it at the end here (as a 'validation' move, like 'cool results, but maybe kids just didn't understand to task? to address this concern, we analyze our attention checks more carefully; and oh look, our results are robust to removing kids who did poorly.')]}


### Comprehension check trials

```{r check-trials, eval = FALSE}

check_trial_data <- d.prod.clean.manual %>%
  filter(competitor_type == "check")

ground_truth_responses <- check_trial_data %>%
  distinct(utterance) %>%
  mutate(unambiguous = utterance %in% c("polar bear", "red flower", "m&ms"),
         wrong = utterance %in% c("cup", "table", "bear"),
         ambiguous = utterance %in% c("dog", "bird", "shirt"))

joined_check_trial_data <- check_trial_data %>%
  left_join(ground_truth_responses, by = "utterance") %>%
  mutate(correct = (unambiguous & button_pressed == 0) | 
           (!unambiguous & button_pressed == 1))
  
subj_check_trial_data <- joined_check_trial_data %>%  
  group_by(age, id) %>%
  summarise(correct = mean(correct))

type_check_trial_data <- joined_check_trial_data %>%
  mutate(check_type = case_when(
    unambiguous ~ "unambiguous",
    wrong ~ "wrong",
    T ~ "ambiguous")) %>%
  group_by(check_type, age, id) %>%
  summarise(correct = mean(correct))

ggplot(type_check_trial_data, aes(x = age, y = correct, color = check_type)) +
  facet_wrap(~ check_type) +
  geom_hline(aes(yintercept = .5), linetype = "dashed") +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
  geom_jitter(width = 0.05, height = 0.05)
  
```


```{r eval = FALSE}
type_check_trial_data %>%
  group_by(age, check_type) %>%
  tidyboot_mean(correct, nboot = 100) %>%
  ggplot(aes(x = age, y = empirical_stat, color = check_type)) +
    facet_wrap(~ check_type) +
    geom_hline(aes(yintercept = .5), linetype = "dashed") +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    #geom_bin_2d(binwidth = c(1, 0.25))
    geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
    geom_point(aes(size = n))

```

```{r plot-check-data, eval = FALSE}
ggplot(subj_check_trial_data, aes(x = age, y = correct)) + 
  geom_jitter() + 
  geom_hline(aes(yintercept = .5), linetype = "dashed")

subj_check_trial_data %>%
  ungroup() %>%
  tidyboot_mean(correct)
```

To assess that children understand the goal of the task, and to aid in our interpretation of any potentially ambiguous responses that children give in earlier trials, we included a check condition for children. We report children's responses on the three types of check trials here.

For all check trials, children were asked to respond "Yes" or "No" as to whether a provided label uniquely identifies one picture out of two. If children are sensitive to informativity of referential expressions, they should respond "Yes" to Close trials with unambiguous labels (e.g., "red flower" to distinguish between a rose and a lily), "No" to Far trials where the provided labels were unambiguous but incorrect (e.g., "table" to distinguish between a pug and a rabbit), and "No" to Close trials with ambiguous expressions (e.g., "dog" to distinguish between a husky and a dalmatian).

We found that children responded as expected to the unambiguous trials. That is, they responded "No" to the incorrect trial ($m=$ `r type_check_trial_data %>% filter(check_type = "wrong") %>% tidyboot_mean(correct)`), and "Yes" to the unambiguous trial ($m=$ `r type_check_trial_data %>% filter(check_type = "unambiguous") %>% tidyboot_mean(correct)`). These findings suggest that children, regardless of age, understood that that goal of the task was to describe a target picture (and not, for instance, to simply say anything).

###Responses by manipulation check performance

```{r}
kid_data <- indiv.measures %>%
  filter(age_group=="kid") 

#predicting context sensitivity by overall manipulation check performance
kid_model_data <- kid_data %>%
  group_by(id, target_type) %>%
  tidyboot_mean(close_far.distance) %>%
  left_join(subj_check_trial_data, by = "id")

kid_model <- kid_model_data %>%
  lm(empirical_stat ~ target_type*correct, data = .) %>%
  tidy() 

ggplot(kid_model_data, aes(x = correct, y = empirical_stat)) +
         geom_jitter() +
         geom_smooth(method = "lm") +
         facet_wrap(~target_type)

#only ambiguous manipulation check trials

ambiguous_check_data <- type_check_trial_data %>%
  filter(check_type == "ambiguous")

kid_ambiguous_model_data <- kid_data %>%
  group_by(id, target_type) %>%
  tidyboot_mean(close_far.distance) %>%
  left_join(ambiguous_check_data, by = "id")

kid_ambiguous_model <- kid_ambiguous_model_data %>%
  lm(empirical_stat ~ target_type*correct, data = .) %>%
  tidy()

ggplot(kid_ambiguous_model_data, aes(x = correct, y = empirical_stat)) +
         geom_point() +
         geom_smooth(method = "lm") +
         facet_wrap(~target_type)

#predicting context sensitivity by age

kid_age_data <- kid_data %>%
  left_join(subj_check_trial_data, by = "id") %>%
  group_by(age, target_type) %>%
  tidyboot_mean(close_far.distance)

kid_age_model <- kid_age_data %>%
  lm(empirical_stat ~ age*target_type, data = .) %>%
  tidy()

ggplot(kid_age_data, aes(x = age, y = empirical_stat)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  facet_wrap(~target_type)
```