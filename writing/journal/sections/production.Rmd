# Experiment 3: Production experiment

In Experiment 2, we found that even young children produced sufficiently informative descriptions of novel tangrams. While children's descriptions were slightly less informative (as indexed by slower reaction times and lower accuracy), they were nonetheless sufficient for naive adult and child listeners to identify the correct tangram. These findings starkly contrast previous research showing that children are poor producers (and identifiers) of informative descriptions [e.g., @anderson1994]. One possible reason for our results is that children, when interacting with their parents, received adequate scaffolding that allowed them to produce informative expressions. That is, parent-child pairs' success in our interactive game could be due to the supportive context of parent scaffolding.

In Experiment 3, we remove the supportive context to ask whether children, as compared to adults, produce referential expressions that are underinformative. We asked adults and children to give short descriptions of familiar and novel objects. Across trials, we varied the context in which target objects appeared. In "close" contexts, the foil object was highly similar to the target, whereas in "far" contexts the foil was dissimilar. In the contexts, a successful referring experession only needs to be a good description of the target. In contrast, in close contexts, a successful referring expression needs to both describe the target well and not describe the foil. For example, a Pug can successfully be called a "dog" if the foil is a table, but not if it is a German Shepherd. 

This experiment allows us to investigate the reasons behind children's apparent failure to coordinate with peers in referential settings. One possibility is that children struggle to produce uniquely identifying expressions, particularly for novel objects.

## Method

```{r e3-anonymize-data, eval = FALSE}
#anonymize
read_csv(here("data/experiment3/dataFromMongo.csv"))%>%
  filter(trial_type == "survey-text") %>%
  filter(iterationName == "pilot2") %>%
  mutate(wID = as.numeric(as.factor(wID))) %>%
  write_csv(here('./data/experiment3/anonymizedDataFromMongo.csv'))
```

```{r e3-pilot-data, eval = FALSE, include = FALSE}
## NOTE: I think ashley manually cleaned this data but we're missing four participants from above
d.prod.clean <- read_csv(here("data/experiment3/prenorming/pilot_data_clean.csv"),
                         show_col_types = FALSE) %>%
  select(-`...1`)
  #filter(utt_length > 1) #%>%
  #mutate(clean_utt = lemmatize_words(clean_utt))
  
```

```{r e3-read-adult-data}
NUM_TRIALS <- 48

d.prod.adult <- read_csv(here("data/experiment3/anonymizedDataFromMongo.csv"),
                       show_col_types = FALSE) %>%
  mutate(age_group = 'adult')
  
complete_games.adult <- d.prod.adult %>%
  group_by(wID) %>%
  tally() %>%
  filter(n >= NUM_TRIALS) %>%
  pull(wID)
```

```{r e3-combine-data, eval = FALSE}
d.prod.kid <- list.files(here("data/experiment3/kid/"), "*.csv",
                          full.names = TRUE) %>%
  map_dfr(read_csv, show_col_types = FALSE) %>%
  mutate(age_group = 'kid') %>%
  rename(wID = id) %>%
  select(-trial_type, -internal_node_id, -trialtype, -test_part, -responses)

d.prod <- d.prod.adult %>%
  filter(wID %in% complete_games.adult) %>%
  group_by(wID) %>%
  slice(1:NUM_TRIALS) %>%
  filter(rt > 1000) %>%
  bind_rows(d.prod.kid) %>%
  unite(id, age_group, wID, remove = F) %>%
  mutate(utterance = str_to_lower(utterance),
         utterance = str_trim(utterance),
         utt_length = str_length(utterance),
         clean_utt = lemmatize_strings(utterance),
         clean_utt = str_remove_all(clean_utt, pattern = ' '))

write_csv(d.prod, here("data/experiment3/production_data_lemmatized.csv"))
```

```{r e3-read-lemmatized_data}
d.prod.clean <- read_csv(
  here("data/experiment3/production_data_lemmatized.csv"),
  show_col_types = FALSE)
```

```{r e3-stim-details}
n_stims <- d.prod.clean %>%
  distinct(target_type, competitor_type, target, foil) 

fam_stims <- n_stims %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()

novel_stims <- n_stims  %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()
```

### Participants

We recruited `r length(complete_games.adult)` adult participants from Amazon Mechanical Turk. All participants gave informed consent prior to the start of the study and were compensated 60Â¢. 

We also recruited \textcolor{red}{XX children} to participate in the study. \textcolor{red}{DEMOGRAPHICS XX}. The study was conducted online over Zoom.

### Stimuli and Design

Stimuli for the game were `r length(fam_stims)` pictures of familiar objects and `r length(novel_stims)` pictures of tangram constructions. Familiar objects were drawn from the set used by @degen2020. Tangrams were drawn from a publicly royalty free set available on \textcolor{red}{XX tangrams site}. Target familiar objects belonged to eight different basic-level categories: bears, birds, cars, candy, dogs, fish, shirts, and tables.

```{r e3-design, out.height = "450px", fig.align = "center", set.cap.width=T, fig.cap = "Objects in Experiment 3 could appear in isolation (A), in a far context (B), or in a close context (C)."}
knitr::include_graphics("figs/e3_diagram.png")
```

For both familiar objects and novel tangrams, trials were divided into three types: isolation trials in which a single target object was presented (Fig. \ref{fig:e3-design}A), far trials in which a target was paired with a competitor of the same type but with low semantic overlap (Fig. \ref{fig:e3-design}B), and close trials in which the target was paired with a competitor with high semantic overlap (Fig. \ref{fig:e3-design}C). For familiar objects, close trials involved two objects in the same basic-level semantic category (e.g. Pug and German Shepherd), and far trials involved two objects in different basic-level categories (e.g. Pug and Rabbit). For novel tangrams, close and far competitors were determined on the basis of their degree of labeling overlap with target in a norming experiment. In this norming experiment, we asked participants to label a number of tangrams in isolation. We used naming agreement--the proportion of responses that were the same for two tangrams--to estimate their semantic similarity [see @zettersten2020]. Details of the novel tangram selection procedure are described in the Supporting Information (\textcolor{red}{actually do this}).

Adults provided labels for a total of eight familiar objects and eight novel tangrams. Each object appeared three times: once in isolation, once in a close context, and once in a far context. All trials of a single type were blocked (e.g. isolation familiar objects), and the six blocks were presented in random order across participants. 

Children provided labels for the same eight familiar and novel objects. To maintain children's attention for the duration of the experiment, they gave responses only in the close and far conditions and not in the isolation condition. Children also participated in an additional check condition designed to measure their understanding of ambiguity in comprehension. The check condition's purpose was to aid in interpretation of potentially ambiguous utterances that children were expected to provide in the Close condition (e.g. saying "bear" to distinguish between a brown bear and a polar bear). In this condition, children were told that they would hear a label produced by another child, and that they should indicate whether this label unambiguously identified the target object. Three of the trials were far trials in which an unambiguously incorrect label was provided to the child (e.g. "table" to distinguish between a pug and a rabbit), three of the trials were close trials in which an unambiguously correct label was provided (e.g. "polar bear" to distinguish between a polar bear and a grizzly bear), and three were close trials in which an ambiguous label was provided (e.g. "dog" to distinguish between a husky and a dalmatian). Children's productions in all conditions were transcribed from video recordings of their participation.

### Procedure

After giving informed consent, adults read a introduction screen informing them that they would be doing an experiment in which they saw one or pictures on the screen at a time. One of these objects would be in a blue box, and they should describe this object as best as they could by typing one or two words into a text box. At the start of each block, they would be told whether they would see one picture (isolation), or two pictures (close and far), and reminded that they should type in a description that would help another participant identify the target in the blue border. Adults then completed all blocks in random order.

After parents gave consent and children assented to participate, children were told that they would see pictures and be asked to describe them. Children then completed several warm-up trials to introduce them to the game. On the first trial, they saw a picture of a red apple. On the next trial, they saw a pear with a blue border around it and the same apple and were asked to describe the object in thge blue box. Then, the experimenter indicated that they would close their eyes on the next trial so that they wouldn't see what children saw. On this trial, children saw a white mug with a blue border around it and a white plate. The blue border disappeared, the experimenter opened their eyes, and the child was encouraged to describe the object in the blue box so that the experimenter could identify it. \textcolor{red}{Ashley's script for the two objects}. A red purple ball and a plant. A white rose and a red rose. A round wooden table and a rectangular wooden table. \textcolor{red}{This was repeated for these other objects I think?}. After these warm-up trials, children were told that they would continue to play this labeling game and that their responses would be told to another person who could see the objects but not which one was in the blue border. In contrast to adults, children always began with a block of familiar objects in either the close or far condition. They always completed the check block last.

## REsults

### Response times reflect familiarity and context

We examined log response times.

```{r e2-rts}
# overall RT distribution
d.prod.clean %>%
  filter(is.na(age)) %>%
  mutate(logged_rt = log(rt)) %>%
  gather(measure, value, logged_rt, rt) %>%
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~ measure, scales='free')

d.prod.clean %>%
  mutate(logged_rt = log(rt)) %>%
  gather(measure, value, logged_rt, rt) %>%
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~ measure, scales='free')
```
  
```{r eval = FALSE}
ggplot(d.prod, aes(x = target_type, y = log(rt), color = competitor_type)) +
  geom_boxplot() +
  theme(legend.position = 'top')
```

```{r, eval = FALSE}
d.prod %>%
  lm(log(rt) ~ target_type * competitor_type, 
     data = .) %>%
  tidy()
```

### Variability in labels 

```{r eval = FALSE}
dodge = position_dodge(0.9)
d.prod %>%
  group_by(target_type, competitor_type) %>%
  tidyboot_mean(utt_length) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat='identity', position=dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position=dodge) +
    labs(x = "target type",
         y = "utterance length") +
    theme(legend.position = 'top')
```

We considered several measures of codability/overlap:

* total number of unique labels (when people overlap a lot, this is smaller because we collect the same number of labels for each target; warning, this could be misleading if we're dropping trials due to high rts and some conditions have more dropped)
* proportion of unique labels to total labels (fixes this problem, but still doesn't account for the distribution of overlap, e.g. if one gets hit 10 times and others get hit 1 vs. everything gets hit 2 times.)
* entropy (we all know and love)
* codeability (what proportion of people provided the most common label, if there were 40 labels and 20 were 'bird', this would be 50%)

```{r eval = FALSE}
d.prod %>%
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) /  mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n)) %>%
  gather(measure, value, unique_labels:codeability) %>%
  group_by(target_type, competitor_type, measure) %>%
  tidyboot_mean(value) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat = 'identity', position=dodge) +
    facet_grid(measure ~. , scales='free') +
    theme(legend.position = "top")

ggsave('variability_measures_clean.pdf', width = 10, height = 10, unit = 'in')

#compare with old data
d.prod.old <- read_csv(here("data/production/pilot_data_clean.csv"))

d.prod.old %>% 
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) / mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n)) %>%
  gather(measure, value, unique_labels:codeability) %>%
  group_by(target_type, competitor_type, measure) %>%
  tidyboot_mean(value) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat = 'identity', position=dodge) +
    facet_grid(measure ~. , scales='free')

```


```{r regression-model-comparing-competitor-types, eval = FALSE}

prod.model <- d.prod %>%
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type, target) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) / mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n))

prod.model %>%
  lmer(codeability ~ target_type*competitor_type + (1|target), data =.) %>%
  tidy() %>%
  filter(effect == "fixed")

```

```{r comparing-distributions, eval = FALSE}
kl_drop <- function(df, x1, x2) {
  x1 <- enquo(x1)
  x2 <- enquo(x2)
  # 
  df %>%
    filter(!!x1 != 0 | !!x2 != 0) %>%
    summarise(kl = KL.shrink(!!x1, !!x2)) %>%
    pull(kl)
}

kls <- d.prod %>%
  count(target_type, competitor_type, target, clean_utt) %>%
  complete(nesting(target_type, target), competitor_type, clean_utt, 
           fill = list(n = 0)) %>%
  pivot_wider(names_from = competitor_type, values_from = n) %>%
  group_by(target_type, target) %>%
  nest() %>%
  mutate(close_far = map(data, ~kl_drop(.x,close, far)),
         isolated_far = map(data, ~kl_drop(.x,isolated, far)),
         isolated_close = map(data, ~kl_drop(.x, close, isolated))) %>%
  unnest(cols = c(close_far, isolated_far, isolated_close)) %>%
  select(-data) %>%
  pivot_longer(cols = c(close_far, isolated_far, isolated_close), 
               names_to = "comparison", values_to = "kl")


ggplot(kls, aes(x = comparison, y = kl)) + 
  facet_wrap(~ target_type) + 
  geom_boxplot()

```

```{r scatterplots, eval = FALSE}

#Look at codeability, etc., as a function of our norming data 
norming_overlap <- read_csv(here("data/norming/jsPsychStims.csv")) %>%
  select(target, overlap_diff) %>%
  right_join(kls, by = "target") %>%
  filter(target_type == "tangram")

ggplot(norming_overlap, aes(x = overlap_diff, y = kl)) +
  geom_point() +
  facet_wrap(~comparison)

```

### Measuring similarity within participant

* within-participant version: For a particular participant, measure whether they use the same word across competition_type conditions for same targets. 

```{r, eval = FALSE}
normalized_lv <- function(s1, s2) {
  lv_dist = stringdist::stringdist(s1, s2, method = 'lv')
  s1_len <- str_length(s1)
  s2_len <- str_length(s2)
  maxlength = pmax(s1_len, s2_len)
  return(lv_dist / maxlength)
}

indiv_measures <- d.prod %>%
  pivot_longer(cols = c(utterance, clean_utt), 
               names_to = "type", values_to = "utterance") %>%
  select(wID, target, target_type, competitor_type, type, utterance) %>%
  group_by(wID, target_type, target, type) %>%
  spread(competitor_type, utterance) %>%
  filter(!is.na(close), !is.na(far), !is.na(isolated)) %>%
  mutate(close_far.distance = normalized_lv(close, far),
         close_isolated.distance = normalized_lv(close, isolated),
         far_isolated.distance = normalized_lv(far, isolated),
         close_far.match = close == far,
         close_isolated.match = close == isolated,
         far_isolated.match = far == isolated) %>%
  gather(pair, distance, close_far.distance:far_isolated.match) %>%
  group_by(target_type, pair, type) %>%
  tidyboot_mean(distance) %>%
  separate(pair, sep = '\\.', into = c('pair', 'measure')) %>%
  arrange(measure)
```


### Tangram label norming 

Our current hypothesis is that we're not getting context effects for adults in this pilot because even the 'close' contexts aren't close along the actual representational dimensions people are using to label things. So if their labeling prior in isolation is to say it's 'sad', that's still highly informative in the close context because the distractor doesn't look sad at all and there's no pressure being put on the prior (kids likely have very different priors, e.g. 'guy' or 'person' with high probability, so these same contexts probably would put pressure on them). To do this in adults, we probably need to elicit priors in isolation for more tangrams (e.g. all 40) and then design better 'close' contexts where we put together tangrams where adults have very similar priors. Maybe the same for kids.

```{r anonymize-norming-data, eval = FALSE}
# anonymize raw csv
read_csv(here('data/norming/dataFromMongo.csv')) %>%
  filter(iterationName %in% c('full_sample', 'full_sample2')) %>%
  filter(trial_type == 'survey-text') %>%
  mutate(wID = as.numeric(as.factor(wID))) %>%
  write_csv(here('./data/norming/anonymizedDataFromMongo.csv'))
```

```{r load-norming-data, eval = FALSE}
# take the first 31 rows of each wID. 
# gets the first game of people who did it twice -- Mongo returns objectIDs in order of insertion date
d.raw.labels <- read_csv(here("data/norming/anonymizedDataFromMongo.csv")) 

initialAssignmentIDs <- d.raw.labels %>%
  group_by(wID) %>%
  summarize(firstAssignmentID = first(aID)) %>%
  pull(firstAssignmentID)

complete_games.norming <- d.raw.labels %>%
  filter(aID %in% initialAssignmentIDs) %>%
  group_by(wID) %>%
  tally() %>%
  filter(n == 31) %>%
  pull(wID)

passed_catch <- d.raw.labels %>%
  filter(target_type == 'catch') %>%
  mutate(utterance = tolower(utterance)) %>%
  filter(utterance == 'strawberry') %>%
  pull(wID)

d <- d.raw.labels %>%
  filter(wID %in% complete_games.norming) %>%
  filter(wID %in% passed_catch) %>%
  filter(target_type != 'catch') %>%
  mutate(clean_utt = tolower(utterance),
         lemmatized_utt = lemmatize_words(clean_utt),
         lemmatized_utt = str_replace_all(lemmatized_utt, regex("\\W+"), ""))
```

How unbalanced is our sample across targets?

```{r eval = FALSE}
d %>%
  group_by(target) %>%
  tally() %>%
  ggplot(aes(x = n)) +
    geom_histogram(bins = 5)
```

check RTs

```{r eval = FALSE}
d %>%
  group_by(wID) %>%
  arrange(trial_index) %>%
  mutate(rep = (lemmatized_utt == lag(lemmatized_utt, 1) &
                lemmatized_utt == lag(lemmatized_utt, 2))) %>%
  filter(rep) %>%
  arrange(wID) 
  # group_by(wID) %>%
  # summarize(meanRT = mean(rt)) %>%
  filter(rt < 2000)
  ggplot(aes(x = meanRT)) +
    geom_histogram(bins = 20)

```

Find most similar sets by JS/KL divergence

```{r get-pair-kl, eval = FALSE}
get_pair_data <- function(df, tangram1, tangram2) {
  df %>%
    filter(target == tangram1) %>%
    select(lemmatized_utt, n) %>%
    rename(target_n = n) %>%
    left_join(df %>%
                filter(target == tangram2) %>%
                select(lemmatized_utt, n),
              by = "lemmatized_utt") %>%
    rename(comparison_n = n)
}

# TODO: take all pairwise comparisons
completed_data <- d %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) 

pairs <- d %>% 
  distinct(target) %>%
  pull() %>%
  combn(2) %>%
  t() %>%
  as_tibble(.name_repair = "unique") %>%
  rename(tangram1 = `...1`, tangram2 = `...2`) %>%
  mutate(target_id = tangram1,
         comparison_id = tangram2) %>%
  group_by(target_id, comparison_id) %>%
  nest() 
```

### Try a simpler, non-KL measure of overlap

```{r overlap-helpers, eval = FALSE}
check_overlap <- function(df, x1, x2) {
  x1 <- enquo(x1)
  x2 <- enquo(x2)
  df %>%
    filter(!!x1 > 0) %>%
    mutate(prop = !!x1 / sum(!!x1)) %>%
    filter(!!x2 > 0) %>%
    summarise(overlap = sum(prop))
}
  
flip_target_competitior <- function(df, x1, x2) {
  x1 <- enquo(x1)
  x2 <- enquo(x2)
  
  df %>%
    mutate(tmp = !!x1, 
           !!x1 := !!x2, 
          !!x2 := tmp) %>%
    select(-tmp)
}
```

```{r simulate-overlap, eval = FALSE}
get_average_overlap <- function(input_data) {
  flipped_pairs <- pairs %>%
    #flip_target_competitior(.,target_id, comparison_id) %>%
    mutate(data = map(data, ~flip_target_competitior(.x, tangram1, tangram2)))
  
  symmetric_overlap <- pairs %>%
    bind_rows(flipped_pairs) %>%
    ungroup() %>%
    # slice_head(n = 1) %>%
    mutate(overlap = map(data, ~ {
      get_pair_data(input_data, .x$tangram1, .x$tangram2) %>%
      check_overlap(., target_n, comparison_n)
    })) %>%
    select(-data) %>%
    unnest(cols = overlap)
  
  symmetric_overlap %>%
    group_by(target_id, comparison_id) %>%
    summarise(overlap = mean(overlap), n= n())
}

ps <- d %>%
  distinct(wID)

first_half <- d %>%
  filter(wID %in% (slice(ps, 1:(n()/2)) %>% pull(wID))) %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) %>%
  get_average_overlap()

second_half <- d %>%
  filter(wID %in% (slice(ps, ((n()/2)+1):(n())) %>% pull(wID))) %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) %>%
  get_average_overlap()

# correlation b/w split data
first_half %>%
  rename(first_half = overlap) %>%
  left_join(second_half, by = c("target_id", "comparison_id", "n")) %>%
  summarise(cor = cor(first_half, overlap, method = "spearman")) %>%
  summarise(cor = mean(cor, na.rm = T))

overall <- d %>%
  #filter(!(target %in% c("R1.jpg", "O1.jpg"))) %>%
  #mutate(target = as.factor(target)) %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) %>%
  get_average_overlap()
```

look at all pairwise comparisons...

```{r plot-overlap, eval = FALSE} 
ggplot(symmetric_overlap, aes(x = target_id, y = comparison_id, fill = log1p(overlap))) + 
  geom_tile() +
  theme(legend.position = 'right')
```

```{r pilot-items, eval = FALSE}
pilot_items <- d.prod.clean %>% 
  filter(target_type == "tangram", competitor_type != "isolated") %>%
  distinct(target, foil, competitor_type) %>%
  rename(target_id = target, comparison_id = foil) %>%
  left_join(symmetric_overlap, by = c("target_id", "comparison_id"))

pilot_items %>%
  group_by(competitor_type) %>%
  tidyboot_mean(overlap)
```

```{r, eval = FALSE}
pilot_items %>%
  ggplot(aes(x = competitor_type, y = overlap)) +
    geom_jitter(width = .1, height = .1) +
    geom_boxplot(alpha = .5)
```

```{r, eval = FALSE}
pairs %>%
  arrange(kl) 
```

highest and lowest overlap pairs
```{r overall overlap, eval = FALSE}

dissimilar <- overall %>%
  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
  arrange(overlap) %>%
  filter(overlap < median(overlap)) %>%
  filter(!duplicated(target_id)) %>%
  mutate(dissimilar = comparison_id,
         dvalue = overlap) %>%
  select(-c(comparison_id, overlap, n))

similar <- overall %>%
  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(overlap > median(overlap)) %>%
  arrange(desc(overlap)) %>%
  filter(!duplicated(target_id)) %>%
  mutate(similar = comparison_id,
         svalue = overlap) %>%
  select(-c(comparison_id, overlap, n))

#TODO: Tangrams that have high and low similarity matches

matches <- left_join(dissimilar, similar, by = "target_id") %>%
  arrange(dvalue, desc(svalue)) %>%
  write.csv(here("data/norming/high_low_matches.csv"))
  
#matches <- overall %>%
#  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
#  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
#  filter(target_id %in% c(dissimilar$target_id, dissimilar$comparison_id) |
#        comparison_id %in% c(dissimilar$target_id, dissimilar$comparison_id)) %>%
#  arrange(desc(overlap))

# function to find targets with high and low similarity matches

#df = NULL

#match_pairs <- function(data, comparison){
#  for (i in 1:data.length) {
#    ifelse (data$target_id %in% c(comparison$target_id, comparison$comparison_id) |
#        comparison_id %in% c(comparison$target_id, comparison$comparison_id), 
#        bind_rows(data[i,], df), return)
#  }
#}



```

### adversarial context design

check trials
```{r check-trials, eval = FALSE}

check_trial_data <- kid_production_data %>%
  filter(trialtype == "check")

ground_truth_responses <- check_trial_data %>%
  distinct(utterance) %>%
  mutate(unambiguous = utterance %in% c("polar bear", "red flower", "M&Ms"),
         wrong = utterance %in% c("cup", "table", "bear"),
         ambiguous = utterance %in% c("dog", "bird", "shirt"))

joined_check_trial_data <- check_trial_data %>%
  left_join(ground_truth_responses, by = "utterance") %>%
  mutate(correct = (unambiguous & button_pressed == 0) | 
           (!unambiguous & button_pressed == 1))
  
subj_check_trial_data <- joined_check_trial_data %>%  
  group_by(age, id) %>%
  summarise(correct = mean(correct))

type_check_trial_data <- joined_check_trial_data %>%
  mutate(check_type = case_when(
    unambiguous ~ "unambiguous",
    wrong ~ "wrong",
    T ~ "ambiguous")) %>%
  group_by(check_type, age, id) %>%
  summarise(correct = mean(correct))

ggplot(type_check_trial_data, aes(x = age, y = correct, color = check_type)) +
  facet_wrap(~ check_type) +
  geom_hline(aes(yintercept = .5), linetype = "dashed") +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
  geom_jitter(width = 0.05, height = 0.05)
  
```


```{r eval = FALSE}
type_check_trial_data %>%
  group_by(age, check_type) %>%
  tidyboot_mean(correct, nboot = 100) %>%
  ggplot(aes(x = age, y = empirical_stat, color = check_type)) +
    facet_wrap(~ check_type) +
    geom_hline(aes(yintercept = .5), linetype = "dashed") +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    #geom_bin_2d(binwidth = c(1, 0.25))
    geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
    geom_point(aes(size = n))

```

```{r plot-check-data, eval = FALSE}
ggplot(subj_check_trial_data, aes(x = age, y = correct)) + 
  geom_jitter() + 
  geom_hline(aes(yintercept = .5), linetype = "dashed")

subj_check_trial_data %>%
  ungroup() %>%
  tidyboot_mean(correct)
```

```{r compare-lenghts, eval = FALSE}
booted_lengths <- kid_production_data %>%
  filter(trialtype == "paired") %>%
  mutate(length = str_count(str_trim(utterance), " ") + 1) %>%
  group_by(age, target_type, competitor_type, id) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)


ggplot(booted_lengths, aes(x = age, y = empirical_stat, 
                           color = competitor_type)) + 
         facet_wrap(~ target_type) + 
  geom_pointrange(aes(ymin = ci_lower, 
                      ymax = ci_upper),
                  position = position_dodge(.5)) + 
  theme(legend.position = "bottom")
```