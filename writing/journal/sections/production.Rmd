# Experiment 3: Production experiment

In Experiment 2, we found that even young children produced sufficiently informative descriptions of novel tangrams. _[rdh: I'm not sure this is how we want to start this section; the main result was that children do *not* produce sufficiently informative descriptions (i.e. that they *comprehend* adult utterances at the same high rate as adult comprehenders, and *comprehend* childrens' utterances at the same low rate as adult comprehenders, so there's clearly a difference in *production*.) I get the point being made here (i.e. that children's descriptions are still above *chance*) but it's a bit subtle compared to that big main effect.]_}
While children's descriptions were slightly less informative (as indexed by slower reaction times and lower accuracy), they were nonetheless sufficient for naive adult and child listeners to identify the correct tangram.
These findings starkly contrast previous research showing that children are poor producers (and identifiers) of informative descriptions [e.g., @anderson1994]. 
One possible reason for our results is that children, when interacting with their parents, received adequate scaffolding that allowed them to produce informative expressions. That is, parent-child pairs' success in our interactive game could be due to the supportive context of parent scaffolding.

In Experiment 3, we remove the supportive context to ask whether children, as compared to adults, produce referential expressions that are underinformative. We asked adults and children to give short descriptions of familiar and novel objects. Across trials, we varied the context in which target objects appeared. In "close" contexts, the foil object was highly similar to the target, whereas in "far" contexts the foil was dissimilar. In the contexts, a successful referring experession only needs to be a good description of the target. In contrast, in close contexts, a successful referring expression needs to both describe the target well and not describe the foil. For example, a Pug can successfully be called a "dog" if the foil is a table, but not if it is a German Shepherd. 

This experiment allows us to investigate the reasons behind children's apparent failure to coordinate with peers in referential settings. One possibility is that children struggle to produce uniquely identifying expressions, particularly for novel objects.

## Methods

```{r e3-anonymize-data, eval = FALSE}
# anonymize
read_csv(here("data/experiment3/dataFromMongo.csv"))%>%
  filter(trial_type == "survey-text") %>%
  filter(iterationName == "pilot2") %>%
  mutate(wID = as.numeric(as.factor(wID))) %>%
  write_csv(here('./data/experiment3/anonymizedDataFromMongo.csv'))
```

```{r e3-pilot-data, eval = FALSE, include = FALSE}
## NOTE: I think ashley manually cleaned this data but we're missing four participants from above
d.prod.clean <- read_csv(here("data/experiment3/prenorming/pilot_data_clean.csv"),
                         show_col_types = FALSE) %>%
  select(-`...1`)
  #filter(utt_length > 1) #%>%
  #mutate(clean_utt = lemmatize_words(clean_utt))
  
```

```{r e3-read-adult-data}
NUM_TRIALS <- 48

d.prod.adult <- read_csv(here("data/experiment3/anonymizedDataFromMongo.csv"),
                       show_col_types = FALSE) %>%
  mutate(age_group = 'adult')
  
complete_games.adult <- d.prod.adult %>%
  group_by(wID) %>%
  tally() %>%
  filter(n >= NUM_TRIALS) %>%
  pull(wID)
```

```{r e3-combine-data, eval = FALSE}
d.prod.kid <- list.files(here("data/experiment3/kid/"), "*.csv",
                          full.names = TRUE) %>%
  map_dfr(read_csv, show_col_types = FALSE) %>%
  mutate(age_group = 'kid') %>%
  rename(wID = id) %>%
  select(-trial_type, -internal_node_id, -trialtype, -test_part, -responses)

d.prod <- d.prod.adult %>%
  filter(wID %in% complete_games.adult) %>%
  group_by(wID) %>%
  slice(1:NUM_TRIALS) %>%
  filter(rt > 1000) %>%
  bind_rows(d.prod.kid) %>%
  unite(id, age_group, wID, remove = F) %>%
  mutate(utterance = str_to_lower(utterance),
         utterance = str_trim(utterance),
         utt_length = str_length(utterance),
         clean_utt = lemmatize_strings(utterance),
         clean_utt = str_remove_all(clean_utt, pattern = ' '))

write_csv(d.prod, here("data/experiment3/production_data_lemmatized.csv"))
```

```{r e3-read-lemmatized_data}
d.prod.clean <- read_csv(
  here("data/experiment3/production_data_lemmatized.csv"),
  show_col_types = FALSE)
```

```{r e3-stim-details}
n_stims <- d.prod.clean %>%
  distinct(target_type, competitor_type, target, foil) 

fam_stims <- n_stims %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()

novel_stims <- n_stims  %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()
```

### Participants

We recruited `r length(complete_games.adult)` adult participants from Amazon Mechanical Turk. All participants gave informed consent prior to the start of the study and were compensated 60Â¢. 

We also recruited \textcolor{red}{XX children} to participate in the study. \textcolor{red}{DEMOGRAPHICS XX}. The study was conducted online over Zoom.

### Stimuli and Design

Stimuli for the game were `r length(fam_stims)` pictures of familiar objects and `r length(novel_stims)` pictures of tangram constructions. Familiar objects were drawn from the set used by @degen2020. Tangrams were drawn from a publicly royalty free set available on \textcolor{red}{XX tangrams site}. Target familiar objects belonged to eight different basic-level categories: bears, birds, cars, candy, dogs, fish, shirts, and tables.

```{r e3-design, out.height = "450px", fig.align = "center", set.cap.width=T, fig.cap = "Objects in Experiment 3 could appear in isolation (A), in a far context (B), or in a close context (C)."}
knitr::include_graphics(here("writing/journal/diagrams/e3_diagram.pdf"))
```

For both familiar objects and novel tangrams, trials were divided into three types: isolation trials in which a single target object was presented (Fig. \ref{fig:e3-design}A), far trials in which a target was paired with a competitor of the same type but with low semantic overlap (Fig. \ref{fig:e3-design}B), and close trials in which the target was paired with a competitor with high semantic overlap (Fig. \ref{fig:e3-design}C). For familiar objects, close trials involved two objects in the same basic-level semantic category (e.g. Pug and German Shepherd), and far trials involved two objects in different basic-level categories (e.g. Pug and Rabbit). For novel tangrams, close and far competitors were determined on the basis of their degree of labeling overlap with target in a norming experiment. In this norming experiment, we asked participants to label a number of tangrams in isolation. We used naming agreement--the proportion of responses that were the same for two tangrams--to estimate their semantic similarity [see @zettersten2020]. Details of the novel tangram selection procedure are described in the Supporting Information.

Adults provided labels for a total of eight familiar objects and eight novel tangrams. Each object appeared three times: once in isolation, once in a close context, and once in a far context. All trials of a single type were blocked (e.g. isolation familiar objects), and the six blocks were presented in random order across participants. 
Children provided labels for the same eight familiar and novel objects. To maintain children's attention for the duration of the experiment, they gave responses only in the close and far conditions and not in the isolation condition. 

Children also participated in an additional check condition designed to measure their understanding of ambiguity in comprehension. The check condition's purpose was to aid in interpretation of potentially ambiguous utterances that children were expected to provide in the Close condition (e.g. saying "bear" to distinguish between a brown bear and a polar bear). In this condition, children were told that they would hear a label produced by another child, and that they should indicate whether this label unambiguously identified the target object. Three of the trials were far trials in which an unambiguously incorrect label was provided to the child (e.g. "table" to distinguish between a pug and a rabbit), three of the trials were close trials in which an unambiguously correct label was provided (e.g. "polar bear" to distinguish between a polar bear and a grizzly bear), and three were close trials in which an ambiguous label was provided (e.g. "dog" to distinguish between a husky and a dalmatian). Children's productions in all conditions were typed into a text box in real time during their participation.

### Procedure

After giving informed consent, adults read a introduction screen informing them that they would be doing an experiment in which they saw one or pictures on the screen at a time. One of these objects would be in a blue box, and they should describe this object as best as they could by typing one or two words into a text box. At the start of each block, they would be told whether they would see one picture (isolation), or two pictures (close and far), and reminded that they should type in a description that would help another participant identify the target in the blue border. Adults then completed all blocks in random order.

After parents gave consent and children assented to participate, children were told that they would see pictures and be asked to describe them. Children then completed several warm-up trials to introduce them to the game. On the first trial, they saw a picture of a red apple. On the next trial, they saw a pear with a blue border around it and the same apple and were asked to describe the object in the blue box. Then, the experimenter indicated that they would close their eyes on the next trial so that they wouldn't see what children saw. On this trial, children saw a white mug with a blue border around it and a white plate. While the experimenter's eyes were closed, the child was encouraged to describe the object in the blue box so that the experimenter could identify it. Once children provided a description, the blue border disappeared, and the experimenter opened their eyes. The experimenter then identified the object based on children's descriptions, and revealed the blue box again to verify whether they selected the correct target. The same procedure was repeated for three more trials with the following image pairs: A red purple ball and a plant; A white rose and a red rose; A round wooden table and a rectangular wooden table. The latter two trials contained object pairs where both objects shared a basic level category label (i.e., "flower" and "table"). These two trials served to signal to children that referential ambiguity is possible. If children did not uniquely identify the target, the experimenter would indicate that the child's expression was ambiguous, and the experimenter would not select the correct target. While these trials provide feedback to the children, the experimenter did not offer alternate descriptions or examples of unambiguous expressions. After these warm-up trials, children were told that they would continue to play this labeling game and that their responses would be told to another person who could see the objects but not which one was in the blue border. In contrast to adults, children always began with a block of familiar objects in either the close or far condition. They always completed the manipulation check block last.

## Results

### Response times reflect familiarity and context

```{r e2-rts}
# overall RT distribution
d.prod.clean %>%
  filter(is.na(age)) %>%
  mutate(logged_rt = log(rt)) %>%
  gather(measure, value, logged_rt, rt) %>%
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~ measure, scales='free')

d.prod.clean %>%
  mutate(logged_rt = log(rt)) %>%
  gather(measure, value, logged_rt, rt) %>%
  ggplot(aes(x = value)) +
    geom_histogram() +
    facet_wrap(~ measure, scales='free')
```
  
```{r compare-lenghts, eval = FALSE}
booted_lengths <- kid_production_data %>%
  filter(trialtype == "paired") %>%
  mutate(length = str_count(str_trim(utterance), " ") + 1) %>%
  group_by(age, target_type, competitor_type, id) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)


ggplot(booted_lengths, aes(x = age, y = empirical_stat, 
                           color = competitor_type)) + 
         facet_wrap(~ target_type) + 
  geom_pointrange(aes(ymin = ci_lower, 
                      ymax = ci_upper),
                  position = position_dodge(.5)) + 
  theme(legend.position = "bottom")
```

```{r eval = FALSE}
ggplot(d.prod.clean%>%filter(age_group=="adult"), aes(x = target_type, y = log(rt), color = competitor_type)) +
  geom_boxplot() +
  theme(legend.position = 'top')
```

```{r, eval = FALSE}
rt_model <- d.prod.clean %>% 
  filter(age_group=="adult") %>%
  lm(log(rt) ~ target_type * competitor_type, 
     data = .) %>%
  tidy()
```

We examined log response times in adults' productions. Children's response times were not analyzed because they responded verbally, and an experimenter entered their responses for them. As such, we do not have accurate measurements of children's actual response times. In the following analysis, we predict response times from target and competitor types and their interactions.

Because objects in close contexts are more similar to one another, we expect longer response times on these trials due to more effort in uniquely identifying the referent. In our sample, adults' response times were longer on close context trials than far and isolated context trials [`r rt_model$`]. Due to the novelty of tangram images, we also expect longer response times on novel trials than on familiar trials. Indeed, we find that adults's response times were longer when describing tangram images than familiar images. In our experiment, response times were measured after participants finished typing their responses and advanced to the next trial, so longer response times may reflect greater effort in generating a description, as well as longer descriptions (and thus longer typing time). However, because all participants were instructed to give brief (two-word) descriptions, it is unlikely that description length contributes to all of the variance in response time differences.

### Variability in labels 

```{r eval = FALSE}
dodge = position_dodge(0.9)
d.prod %>%
  group_by(target_type, competitor_type) %>%
  tidyboot_mean(utt_length) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat='identity', position=dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position=dodge) +
    labs(x = "target type",
         y = "utterance length") +
    theme(legend.position = 'top')
```

We considered several measures of codability/overlap:

* total number of unique labels (when people overlap a lot, this is smaller because we collect the same number of labels for each target; warning, this could be misleading if we're dropping trials due to high rts and some conditions have more dropped)
* proportion of unique labels to total labels (fixes this problem, but still doesn't account for the distribution of overlap, e.g. if one gets hit 10 times and others get hit 1 vs. everything gets hit 2 times.)
* entropy (we all know and love)
* codeability (what proportion of people provided the most common label, if there were 40 labels and 20 were 'bird', this would be 50%)

```{r eval = FALSE}
d.prod %>%
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) /  mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n)) %>%
  gather(measure, value, unique_labels:codeability) %>%
  group_by(target_type, competitor_type, measure) %>%
  tidyboot_mean(value) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat = 'identity', position=dodge) +
    facet_grid(measure ~. , scales='free') +
    theme(legend.position = "top")

ggsave('variability_measures_clean.pdf', width = 10, height = 10, unit = 'in')

#compare with old data
d.prod.old <- read_csv(here("data/production/pilot_data_clean.csv"))

d.prod.old %>% 
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) / mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n)) %>%
  gather(measure, value, unique_labels:codeability) %>%
  group_by(target_type, competitor_type, measure) %>%
  tidyboot_mean(value) %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = competitor_type)) +
    geom_bar(stat = 'identity', position=dodge) +
    facet_grid(measure ~. , scales='free')

```


```{r regression-model-comparing-competitor-types, eval = FALSE}

prod.model <- d.prod %>%
  group_by(target_type, competitor_type, utterance, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(target_type, competitor_type, target) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) / mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n))

prod.model %>%
  lmer(codeability ~ target_type*competitor_type + (1|target), data =.) %>%
  tidy() %>%
  filter(effect == "fixed")

```

```{r comparing-distributions, eval = FALSE}
kl_drop <- function(df, x1, x2) {
  x1 <- enquo(x1)
  x2 <- enquo(x2)
  # 
  df %>%
    filter(!!x1 != 0 | !!x2 != 0) %>%
    summarise(kl = KL.shrink(!!x1, !!x2)) %>%
    pull(kl)
}

kls <- d.prod %>%
  count(target_type, competitor_type, target, clean_utt) %>%
  complete(nesting(target_type, target), competitor_type, clean_utt, 
           fill = list(n = 0)) %>%
  pivot_wider(names_from = competitor_type, values_from = n) %>%
  group_by(target_type, target) %>%
  nest() %>%
  mutate(close_far = map(data, ~kl_drop(.x,close, far)),
         isolated_far = map(data, ~kl_drop(.x,isolated, far)),
         isolated_close = map(data, ~kl_drop(.x, close, isolated))) %>%
  unnest(cols = c(close_far, isolated_far, isolated_close)) %>%
  select(-data) %>%
  pivot_longer(cols = c(close_far, isolated_far, isolated_close), 
               names_to = "comparison", values_to = "kl")


ggplot(kls, aes(x = comparison, y = kl)) + 
  facet_wrap(~ target_type) + 
  geom_boxplot()

```

```{r scatterplots, eval = FALSE}

#Look at codeability, etc., as a function of our norming data 
norming_overlap <- read_csv(here("data/norming/jsPsychStims.csv")) %>%
  select(target, overlap_diff) %>%
  right_join(kls, by = "target") %>%
  filter(target_type == "tangram")

ggplot(norming_overlap, aes(x = overlap_diff, y = kl)) +
  geom_point() +
  facet_wrap(~comparison)

```

### Measuring similarity within participant

* within-participant version: For a particular participant, measure whether they use the same word across competition_type conditions for same targets. 

```{r, eval = FALSE}
normalized_lv <- function(s1, s2) {
  lv_dist = stringdist::stringdist(s1, s2, method = 'lv')
  s1_len <- str_length(s1)
  s2_len <- str_length(s2)
  maxlength = pmax(s1_len, s2_len)
  return(lv_dist / maxlength)
}

indiv_measures <- d.prod %>%
  pivot_longer(cols = c(utterance, clean_utt), 
               names_to = "type", values_to = "utterance") %>%
  select(wID, target, target_type, competitor_type, type, utterance) %>%
  group_by(wID, target_type, target, type) %>%
  spread(competitor_type, utterance) %>%
  filter(!is.na(close), !is.na(far), !is.na(isolated)) %>%
  mutate(close_far.distance = normalized_lv(close, far),
         close_isolated.distance = normalized_lv(close, isolated),
         far_isolated.distance = normalized_lv(far, isolated),
         close_far.match = close == far,
         close_isolated.match = close == isolated,
         far_isolated.match = far == isolated) %>%
  gather(pair, distance, close_far.distance:far_isolated.match) %>%
  group_by(target_type, pair, type) %>%
  tidyboot_mean(distance) %>%
  separate(pair, sep = '\\.', into = c('pair', 'measure')) %>%
  arrange(measure)
```



Find most similar sets by JS/KL divergence

```{r get-pair-kl, eval = FALSE}
get_pair_data <- function(df, tangram1, tangram2) {
  df %>%
    filter(target == tangram1) %>%
    select(lemmatized_utt, n) %>%
    rename(target_n = n) %>%
    left_join(df %>%
                filter(target == tangram2) %>%
                select(lemmatized_utt, n),
              by = "lemmatized_utt") %>%
    rename(comparison_n = n)
}

# TODO: take all pairwise comparisons
completed_data <- d %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) 

pairs <- d %>% 
  distinct(target) %>%
  pull() %>%
  combn(2) %>%
  t() %>%
  as_tibble(.name_repair = "unique") %>%
  rename(tangram1 = `...1`, tangram2 = `...2`) %>%
  mutate(target_id = tangram1,
         comparison_id = tangram2) %>%
  group_by(target_id, comparison_id) %>%
  nest() 
```

### Try a simpler, non-KL measure of overlap

```{r overlap-helpers, eval = FALSE}
check_overlap <- function(df, x1, x2) {
  x1 <- enquo(x1)
  x2 <- enquo(x2)
  df %>%
    filter(!!x1 > 0) %>%
    mutate(prop = !!x1 / sum(!!x1)) %>%
    filter(!!x2 > 0) %>%
    summarise(overlap = sum(prop))
}
  
flip_target_competitior <- function(df, x1, x2) {
  x1 <- enquo(x1)
  x2 <- enquo(x2)
  
  df %>%
    mutate(tmp = !!x1, 
           !!x1 := !!x2, 
          !!x2 := tmp) %>%
    select(-tmp)
}
```

```{r simulate-overlap, eval = FALSE}
get_average_overlap <- function(input_data) {
  flipped_pairs <- pairs %>%
    #flip_target_competitior(.,target_id, comparison_id) %>%
    mutate(data = map(data, ~flip_target_competitior(.x, tangram1, tangram2)))
  
  symmetric_overlap <- pairs %>%
    bind_rows(flipped_pairs) %>%
    ungroup() %>%
    # slice_head(n = 1) %>%
    mutate(overlap = map(data, ~ {
      get_pair_data(input_data, .x$tangram1, .x$tangram2) %>%
      check_overlap(., target_n, comparison_n)
    })) %>%
    select(-data) %>%
    unnest(cols = overlap)
  
  symmetric_overlap %>%
    group_by(target_id, comparison_id) %>%
    summarise(overlap = mean(overlap), n= n())
}

ps <- d %>%
  distinct(wID)

first_half <- d %>%
  filter(wID %in% (slice(ps, 1:(n()/2)) %>% pull(wID))) %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) %>%
  get_average_overlap()

second_half <- d %>%
  filter(wID %in% (slice(ps, ((n()/2)+1):(n())) %>% pull(wID))) %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) %>%
  get_average_overlap()

# correlation b/w split data
first_half %>%
  rename(first_half = overlap) %>%
  left_join(second_half, by = c("target_id", "comparison_id", "n")) %>%
  summarise(cor = cor(first_half, overlap, method = "spearman")) %>%
  summarise(cor = mean(cor, na.rm = T))

overall <- d %>%
  #filter(!(target %in% c("R1.jpg", "O1.jpg"))) %>%
  #mutate(target = as.factor(target)) %>%
  count(target, lemmatized_utt) %>%
  complete(target, lemmatized_utt, 
           fill = list(n = 0)) %>%
  get_average_overlap()
```

look at all pairwise comparisons...

```{r plot-overlap, eval = FALSE} 
ggplot(symmetric_overlap, aes(x = target_id, y = comparison_id, fill = log1p(overlap))) + 
  geom_tile() +
  theme(legend.position = 'right')
```

```{r pilot-items, eval = FALSE}
pilot_items <- d.prod.clean %>% 
  filter(target_type == "tangram", competitor_type != "isolated") %>%
  distinct(target, foil, competitor_type) %>%
  rename(target_id = target, comparison_id = foil) %>%
  left_join(symmetric_overlap, by = c("target_id", "comparison_id"))

pilot_items %>%
  group_by(competitor_type) %>%
  tidyboot_mean(overlap)
```

```{r, eval = FALSE}
pilot_items %>%
  ggplot(aes(x = competitor_type, y = overlap)) +
    geom_jitter(width = .1, height = .1) +
    geom_boxplot(alpha = .5)
```

```{r, eval = FALSE}
pairs %>%
  arrange(kl) 
```

highest and lowest overlap pairs
```{r overall overlap, eval = FALSE}

dissimilar <- overall %>%
  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
  arrange(overlap) %>%
  filter(overlap < median(overlap)) %>%
  filter(!duplicated(target_id)) %>%
  mutate(dissimilar = comparison_id,
         dvalue = overlap) %>%
  select(-c(comparison_id, overlap, n))

similar <- overall %>%
  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
  filter(overlap > median(overlap)) %>%
  arrange(desc(overlap)) %>%
  filter(!duplicated(target_id)) %>%
  mutate(similar = comparison_id,
         svalue = overlap) %>%
  select(-c(comparison_id, overlap, n))

#TODO: Tangrams that have high and low similarity matches

matches <- left_join(dissimilar, similar, by = "target_id") %>%
  arrange(dvalue, desc(svalue)) %>%
  write.csv(here("data/norming/high_low_matches.csv"))
  
#matches <- overall %>%
#  filter(!(target_id %in% c("R1.jpg", "O1.jpg"))) %>%
#  filter(!(comparison_id %in% c("R1.jpg", "O1.jpg"))) %>%
#  filter(target_id %in% c(dissimilar$target_id, dissimilar$comparison_id) |
#        comparison_id %in% c(dissimilar$target_id, dissimilar$comparison_id)) %>%
#  arrange(desc(overlap))

# function to find targets with high and low similarity matches

#df = NULL

#match_pairs <- function(data, comparison){
#  for (i in 1:data.length) {
#    ifelse (data$target_id %in% c(comparison$target_id, comparison$comparison_id) |
#        comparison_id %in% c(comparison$target_id, comparison$comparison_id), 
#        bind_rows(data[i,], df), return)
#  }
#}



```

### Comprehension check trials

```{r check-trials, eval = FALSE}

check_trial_data <- kid_production_data %>%
  filter(trialtype == "check")

ground_truth_responses <- check_trial_data %>%
  distinct(utterance) %>%
  mutate(unambiguous = utterance %in% c("polar bear", "red flower", "M&Ms"),
         wrong = utterance %in% c("cup", "table", "bear"),
         ambiguous = utterance %in% c("dog", "bird", "shirt"))

joined_check_trial_data <- check_trial_data %>%
  left_join(ground_truth_responses, by = "utterance") %>%
  mutate(correct = (unambiguous & button_pressed == 0) | 
           (!unambiguous & button_pressed == 1))
  
subj_check_trial_data <- joined_check_trial_data %>%  
  group_by(age, id) %>%
  summarise(correct = mean(correct))

type_check_trial_data <- joined_check_trial_data %>%
  mutate(check_type = case_when(
    unambiguous ~ "unambiguous",
    wrong ~ "wrong",
    T ~ "ambiguous")) %>%
  group_by(check_type, age, id) %>%
  summarise(correct = mean(correct))

ggplot(type_check_trial_data, aes(x = age, y = correct, color = check_type)) +
  facet_wrap(~ check_type) +
  geom_hline(aes(yintercept = .5), linetype = "dashed") +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
  geom_jitter(width = 0.05, height = 0.05)
  
```


```{r eval = FALSE}
type_check_trial_data %>%
  group_by(age, check_type) %>%
  tidyboot_mean(correct, nboot = 100) %>%
  ggplot(aes(x = age, y = empirical_stat, color = check_type)) +
    facet_wrap(~ check_type) +
    geom_hline(aes(yintercept = .5), linetype = "dashed") +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    #geom_bin_2d(binwidth = c(1, 0.25))
    geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
    geom_point(aes(size = n))

```

```{r plot-check-data, eval = FALSE}
ggplot(subj_check_trial_data, aes(x = age, y = correct)) + 
  geom_jitter() + 
  geom_hline(aes(yintercept = .5), linetype = "dashed")

subj_check_trial_data %>%
  ungroup() %>%
  tidyboot_mean(correct)
```

