# Experiment 3: Production experiment

In Experiment 2, we found that naive children and adults were able to comprehend referential expressions equally well. 
Contrary to the classical view that young children are rigidly fixating on a single way of conceptualizing the tangram, even young children seem remarkably flexible in their ability to accommodate different descriptions.
That is, to the extent that children contributed to poorer group performance in Experiment 1, their contribution is not well-explained by the *comprehension hypothesis*. At the same time, however, we found some evidence consistent with the *production hypothesis*: messages originally produced by children were somewhat less comprehensible for listeners of any age (as indexed by slower reaction times and lower accuracy).

While this source effect is intriguing, it is difficult to disentangle the original messages from the interactive parent-child context in which they were produced. 
For example, it is possible that children were receiving interactive scaffolding that prompted them toward more comprehensible expressions. 
Or, conversely, it is possible that children were relying on their parents to take on more of the division of labor of interpretation [@hawkins2021division] and were thus producing expressions that are *less* comprehensible to naive audience than they were actually capable of.
In Experiment 3, we remove the interactive context to more directly assess children's ability to produce referential expressions for novel tangram objects in different contexts. 
We used a 2x2 design aiming to tease apart two related explanations for poor production performance.
First, to assess the extent to which production difficulties stem from *pragmatic reasoning* (i.e. the ability to recognize that an accessible label is not sufficiently informative in context), we manipulate whether the distractor in context is more or less similar to the target. 
Second, to assess the extent to which production difficulties stem from impoverished *lexical priors* (i.e. the ability to access candidate labels for a given referent), we manipulate whether the target objects are familiar photographs or novel tangram shapes [see @horton2002speakers]. 
To the extent that children fail to produce context-sensitive utterances for familiar objects with accessible labels, we may expect that pragmatic reasoning is the primary bottleneck on performance.
To the extent that there is greater overall variance in the utterances produced by children, we may expect that lexical priors play a larger role.

## Methods

```{r plot-colors}
CHILDREN <- "#08519c"
ADULTS <- "#fe9929"
```

```{r e3-anonymize-data, eval = FALSE}
# this block pre-processes the raw, un-anonymized data from adults
# we keep it here for reproducibility but only release the preprocessed data
# read_csv(here("data/experiment3/dataFromMongo.csv"))%>%
#   filter(trial_type == "survey-text") %>%
#   filter(iterationName == "pilot2") %>%
#   mutate(wID = as.numeric(as.factor(wID))) %>%
#   write_csv(here('./data/experiment3/anonymizedDataFromMongo.csv'))
```

```{r e3-read-adult-data}
NUM_TRIALS <- 48
d.prod.adult <- here("data/experiment3/anonymizedDataFromMongo.csv") %>%
  read_csv(show_col_types = FALSE) %>%
  mutate(age_group = 'adult')
  
complete_games.adult <- d.prod.adult %>%
  group_by(wID) %>%
  tally() %>%
  filter(n >= NUM_TRIALS) %>%
  pull(wID)
```

```{r e3-combine-data, eval = FALSE}
# this block pre-processes the raw, un-anonymized data from kids
# we keep it here for reproducibility but only release the preprocessed data for privacy

# d.prod.kid <- list.files(here("data/experiment3/kid/"), "*.csv",
#                           full.names = TRUE) %>%
#   map_dfr(read_csv, show_col_types = FALSE) %>%
#   mutate(age_group = 'kid') %>%
#   rename(wID = id) %>%
#   select(-trial_type, -internal_node_id, -trialtype, -test_part, -responses)
# 
# d.prod <- d.prod.adult %>%
#   filter(wID %in% complete_games.adult) %>%
#   group_by(wID) %>%
#   slice(1:NUM_TRIALS) %>%
#   filter(rt > 1000) %>%
#   bind_rows(d.prod.kid) %>%
#   unite(id, age_group, wID, remove = F) %>%
#   mutate(utterance = str_to_lower(utterance),
#          utterance = str_trim(utterance),
#          utt_length = str_length(utterance),
#          clean_utt = lemmatize_strings(utterance),
#          clean_utt = str_remove_all(clean_utt, pattern = ' '))
# 
# write_csv(d.prod, here("data/experiment3/production_data_lemmatized.csv"))
```

```{r e3-read-lemmatized_data}
# manually cleaned data

# missing_data <- read_csv(
#   here("data/experiment3/missing_production_data_cleaned.csv")) %>%
#   mutate(button_pressed = as.numeric(button_pressed))

# d.prod.clean.merge <- read_csv(
#   here("data/experiment3/production_data_lemmatized_cleaned.csv"),
#   guess_max = 5000) %>%
#   filter(!(id %in% c('adult_49', 'kid_40'))) %>% # said I don't know to everything / random
#   mutate(button_pressed = as.numeric(button_pressed)) %>%
#   bind_rows(missing_data)

# write_csv(d.prod.clean.merge, here("data/experiment3/production_data_cleaned.csv"))

d.prod.clean.manual <- read_csv(here("data/experiment3/production_data_cleaned.csv"))

# kid descriptives
n_kids <- d.prod.clean.manual %>%
  filter(age_group == "kid") %>%
  {unique(.$id)}

kid_age <- d.prod.clean.manual %>%
  filter(age_group == "kid") %>%
  filter(!duplicated(id))

```

```{r e3-stim-details}
n_stims <- d.prod.clean.manual %>%
  distinct(target_type, competitor_type, target, foil) 

fam_stims <- n_stims %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()

novel_stims <- n_stims  %>%
  filter(target_type == "familiar") %>%
  distinct(target, foil) %>%
  unlist() %>%
  unique()
```

### Participants

We recruited `r length(complete_games.adult)` adult participants from Amazon Mechanical Turk. All participants gave informed consent prior to the start of the study and were compensated 60Â¢. 
We also recruited `r length(n_kids)` children aged 4 to 8 years old ($M=$ `r mean(kid_age$age)`) to participate in the study. The study was conducted online over Zoom. Parents provided informed written and verbal consent, and children provided verbal consent. Families were compensated $5 for their participation.

### Stimuli and Design

We used `r length(fam_stims)` pictures of familiar objects and `r length(novel_stims)` pictures of tangrams. 
Familiar objects were drawn from an image set used by @degen2020. We used eight different basic-level categories that we expected to be familiar to children: bears, birds, cars, candy, dogs, fish, shirts, and tables.
Tangrams were drawn from a publicly royalty free set available on https://www.1001freedownloads.com/.

```{r e3-design, out.width = "0.6\\textwidth", fig.align = "center", set.cap.width=T, fig.cap = "Objects in Experiment 3 could appear in isolation (A), in a far context (B), or in a close context (C)."}
knitr::include_graphics(here("writing/journal/diagrams/e3_diagram.pdf"))
```

For both familiar objects and novel tangrams, trials were divided into three types: isolation trials in which a single target object was presented (Figure \ref{fig:e3-design}A), far trials in which a target was paired with a competitor of the same type but with low semantic overlap (Figure \ref{fig:e3-design}B), and close trials in which the target was paired with a competitor with high semantic overlap (Figure \ref{fig:e3-design}C). For familiar objects, close trials involved two objects in the same basic-level semantic category (e.g., Pug and German Shepherd), and far trials involved two objects in different basic-level categories (e.g., Pug and Rabbit). For novel tangrams, close and far competitors were determined on the basis of their degree of labeling overlap with target in a norming experiment. In this norming experiment, we asked adult participants to produce labels for a number of tangrams in isolation. We used naming agreement--the proportion of responses that were the same for two tangrams--to estimate their semantic similarity [see @zettersten2020], and then constructed close and far contexts based on high or low similarity, respectively. Details of the novel tangram selection procedure are described in the Supplemental Information.

Adults provided labels for a total of eight familiar objects and eight novel tangrams. Each object appeared three times: once in isolation, once in a close context, and once in a far context. All trials of a single type were presented in a single block (e.g. six trials in a row of 'familiar' objects in 'close' contexts), and the six blocks were presented in random order across participants.
Children provided labels for the same eight familiar and novel objects. To maintain children's attention for the duration of the experiment, they gave responses only in the close and far conditions and not in the isolation condition. Children's productions in all conditions were typed into a text box in real time by an experimenter during their participation.

Children also participated in an additional task designed to measure individual differences understanding ambiguity in *comprehension* [@markman1977realizing; @beal1990development], as failures to judge the informativity of others' expressions may be implicated in failures to produce appropriately informative utterances of one's own in the close condition (e.g. saying "bear" when both the target and distractor are bears).
In this condition, children were told that they would hear a label produced by another child, and that they should indicate whether this label adequately identified the target object.
Three of the trials were far trials in which an unambiguously incorrect label was provided to the child (e.g. "table" to distinguish between a pug and a rabbit), three of the trials were close trials in which an unambiguously correct label was provided (e.g. "polar bear" to distinguish between a polar bear and a grizzly bear), and three were close trials in which an ambiguous label was provided (e.g. "dog" to distinguish between a husky and a dalmatian).
Children responded verbally ("yes" it is adequate or "no") and an experimenter selected the corresponding button on the screen.

### Procedure

After giving informed consent, adults read an introduction screen informing them that they would be doing an experiment in which they saw one or two pictures on the screen at a time. One of these objects would be in a blue box, and they should describe this object as best as they could by typing one or two words into a text box. At the start of each block, they would be told whether they would see one picture (isolation), or two pictures (close and far), and reminded that they should type in a description that would help another participant identify the target with a blue border. Adults then completed all blocks in random order.

After parents gave consent and children assented to participate, children were told that they would see pictures and be asked to describe them. Children then completed several warm-up trials to introduce them to the game. On the first trial, they saw a picture of a red apple. On the next trial, they saw a pear with a blue border around it and the same apple and were asked to describe the object in the blue box. Then, the experimenter indicated that they would close their eyes on the next trial so that they wouldn't see what children saw. On this trial, children saw a white mug with a blue border around it and a white plate. While the experimenter's eyes were closed, the child was encouraged to describe the object in the blue box so that the experimenter could identify it. Once children provided a description, the blue border disappeared, and the experimenter opened their eyes. The experimenter then identified the object based on children's descriptions, and revealed the blue box again to verify whether they selected the correct target. 

The same procedure was repeated for three more trials with the following image pairs: a red purple ball and a plant; a white rose and a red rose; a round wooden table and a rectangular wooden table. The latter two trials contained object pairs where both objects shared a basic level category label (i.e., "flower" and "table"). These two trials served to subtly indicate to children that referential ambiguity is possible. If children did not uniquely identify the target (e.g. just saying "flower"), the experimenter would indicate that the child's expression was ambiguous, and the experimenter would not select the correct target. While these trials provided feedback about ambiguity to the children, the experimenter did not suggest alternate descriptions or examples of unambiguous expressions. After these warm-up trials, children were told that they would continue to play the labeling game and that their responses would be shown to another person who did not know which one was in the blue border. In contrast to adults, children always began with a block of familiar objects in either the close or far condition. They always completed the manipulation check block last.

### Preprocessing 

We cleaned the text input using a uniform rubric across the combined data set.
First, we manually corrected typos and removed stop words (e.g. determiners like 'a', 'the').
Second, we lemmatized all entries to remove spurious differences between tenses and plurals of the same root form. The measures we use in analysis consider edit distance, which may be aritifically inflated due to word order (e.g., the edit distance between "running person" and "person running" would be high, despite conceptual and semantic similarity between the descriptions). As such, we manually cleaned the data to remove similar phrases or structures across descriptions (e.g., if a participant repeats "a person who is..." on every trial, we removed that phrase). We use the manually cleaned data for our analyses, but pre-cleaned, lemmatized data can be found on our OSF page (https://osf.io/vkug8/).
Third, we removed spaces and collapsed multiple words together into a single token (e.g. 'German Shepherd' was tokenized to 'germanshepherd'). While adult participants typed in their own responses, children's responses were entered by an experimenter. When children's descriptions were overly long, experimenters prompted children to simplify them by asking, "Can you say that in one or two words?". Children were only prompted once, regardless of whether they simplified their expression. 

## Results

We focus on evaluating two primary hypotheses.
First, could children be failing to take into account the referential context when deciding what to say, leading to more ambiguous or underinformative referring expressions?
Second, could children simply have more uncertainty over possible acceptable labels for novel objects, making retrieval challenging [@lachman1974language; @cycowicz1997picture]?
These hypotheses are not mutually exclusive. 
Indeed, the corresponding mechanisms -- *pragmatic reasoning* and *lexical priors* -- are both implicated in recent production models [e.g. @murthy2021shades, @hawkins2022partners]. 

### Children are differentially sensitive to referential context

```{r context-sensitivity, fig.pos = "t", fig.align = "center", fig.width = 10, fig.cap = "\\label{fig:e3_contextsensitivity} Proportion of descriptions of targets that were exact matches across the close and far contexts (error bars are 95\\% CIs). Children were twice as likely to give the same description for familiar objects than tangrams across contexts, while adults equally modulated their descriptions for both target types."}

# we can just look at the item-by-item / speaker-by-speaker level 
# where there's a context effect...
indiv.measures <- d.prod.clean.manual %>%
  filter(competitor_type != 'check') %>%
  group_by(age_group, id, target_type, competitor_type, target, clean_utt) %>%
  tally() %>%
  group_by(age_group, id, target) %>%
  pivot_wider(names_from = competitor_type, values_from = c('clean_utt')) %>%
  unnest(cols = c(close, far, isolated)) %>%
  filter(!is.na(close), !is.na(far)) %>%
  mutate(close_far.distance = helpers$normalized_lv(close, far),
         close_isolated.distance = helpers$normalized_lv(close, isolated),
         far_isolated.distance = helpers$normalized_lv(far, isolated),
         far_close_overlap = far == close,
         far_isolated_overlap = far == isolated,
         close_isolated_overlap = close == isolated) 

#measure context sensitivity by exact matches across conditions
contextsensitivity.means <- indiv.measures %>%
  group_by(age_group, target_type) %>%
  tidyboot_mean(far_close_overlap, na.rm = T) 

dodge = position_dodge(0.9)

contextsensitivity.means %>%
  ggplot(aes(x = target_type, y = empirical_stat, fill = age_group)) +
    geom_bar(stat = 'identity', position = dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, 
                  position = dodge) +
    scale_fill_manual(values = c(ADULTS, CHILDREN)) +
    labs(y = 'Proportion of exact matches in descriptions', fill = "Age group") +
    theme(aspect.ratio = 1, legend.position = "right")
```

```{r contextsensitivitylmer}
indiv.measures.lmer <- indiv.measures %>%
  ungroup() %>%
  mutate(utt_length = (str_length(close) + str_length(far)) / 2) %>%
  glmer(far_close_overlap ~ age_group * target_type + scale(utt_length) + (1 | id),
        family = 'binomial',
        data = .) %>%
  tidy() %>%
  filter(effect == 'fixed') %>%
  select(-group, -effect)

indiv.measures.softmeasure <- indiv.measures %>%
  mutate(utt_length = (str_length(close) + str_length(far)) / 2) %>%
  lmer(close_far.distance ~ age_group * target_type + (1 | target) + (1 + target_type | id),
        data = .) %>%
  tidy() %>%
  filter(effect == 'fixed') %>%
  select(-group, -effect)

indiv.measures.inter <- indiv.measures.lmer %>% filter(term == 'age_groupkid:target_typetangram')
soft.inter <- indiv.measures.softmeasure %>% filter(term == 'age_groupkid:target_typetangram')

```

To test our first hypothesis, we examine the extent to which the same participant produces different utterances across the *far* vs. *close* contexts.
We begin by considering a simple 'exact match' criterion, coding an item as 1 if the participant used the same label for that item in both contexts and 0 if they used different labels. 
This criterion is conservative in the sense that it will miss a number of near- or partial-matches, giving a lower-bound for overlap.
We model the binary variable of context overlap using a mixed-effects logistic regression.
We include fixed effects for age cohort (child vs. adult) and target type (familiar vs. novel), as well as their interaction.
To control for the fact that longer utterance strings are less likely to exactly match by chance, we also include a term for the average length of the close and far labels for that speaker and item. 
The most complex random effects structure that converged only included random intercepts at the participant level.

We found a significant interaction between age group and target type, $b=`r indiv.measures.inter$estimate`, z = `r indiv.measures.inter$statistic`, p < 0.001$ (see Figure  \ref{fig:e3_contextsensitivity}). 
Although adults displayed similar rates of context-sensitivity for familiar and novel objects (familiar: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'adult', target_type == 'familiar'))$empirical_stat`, novel: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'adult', target_type == 'tangram'))$empirical_stat`), children were nearly twice as likely to provide the exact same label across contexts for a familiar object (familiar: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'kid', target_type == 'tangram'))$empirical_stat`; novel: $m=$ `r (contextsensitivity.means %>% filter(age_group == 'kid', target_type == 'familiar'))$empirical_stat`).
Similar results were obtained using 'softer' measures like edit distance, which is the number of edits required to turn one string into the other, $b= `r soft.inter$estimate`, z = `r soft.inter$statistic`, p < 0.001$ (see Supplemental Figure \ref{fig:Levensthein_appendix}). 
In other words, while adults appropriately modulated their utterances across contexts, children often produced the same description for familiar targets across contexts, even when it was not sufficient to distinguish the target from the distactor. 

### Familiar objects elicit less variable names

```{r fig.align='center', fig.cap="\\label{fig:e3_variability} Proportion of descriptions that were unique labels (error bars are 95\\% CIs). Across adult and child participants, descriptions of familiar images were more likely to overlap."}
var.measures <- d.prod.clean.manual %>%
  filter(competitor_type == "far") %>%
  group_by(age_group, target_type, competitor_type, clean_utt, target) %>%
  tally() %>%
  arrange(target) %>%
  group_by(age_group, target_type, target) %>%
  mutate(total_labels = sum(n)) %>%
  summarize(unique_labels = length(n), 
            normalized_unique_labels = mean(unique_labels) / mean(total_labels),
            entropy = entropy(n, method="SG"),
            normalized_entropy = mean(entropy) / log(length(n)),
            codeability = max(n) / sum(n)) %>%
  select(-entropy, -unique_labels) %>%
  gather(measure, value, normalized_unique_labels:codeability)

var.plot <- var.measures %>%
  group_by(age_group, target_type, measure) %>%
  tidyboot_mean(value)

var.plot %>% 
  filter(measure == 'normalized_unique_labels') %>% # 
  ggplot(aes(x = target_type, y = empirical_stat, fill = age_group)) +
    geom_bar(stat = 'identity', position=dodge) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = dodge) +
    scale_fill_manual(values = c(ADULTS, CHILDREN)) +
    facet_grid( ~ . , scales='free') +
    labs(x = '', y = 'proportion unique labels') +
    theme(legend.position = "top")

#ggsave('variability_measures.pdf', width = 10, height = 10, unit = 'in')

```

```{r regression-model-comparing-competitor-types}
var.out.means <- var.measures %>%
  filter(measure == 'normalized_unique_labels') %>% 
  group_by(target_type, age_group) %>%
  summarize(m = mean(value)) %>%
  pivot_wider(names_from = c('target_type', 'age_group'), values_from = m)

var.out <- var.measures %>%
  filter(measure == 'normalized_unique_labels') %>% # 
  lmer(value ~ target_type * age_group + (1 | target), data =., contrasts = list(age_group = contr.sum(2), target_type = contr.sum(2))) %>%
  tidy() %>%
  filter(effect == "fixed")

var.out.entropy <- var.measures %>%
  filter(measure == 'normalized_entropy') %>% # 
  lmer(value ~ target_type * age_group + (1 | target), data =., contrasts = list(age_group = contr.sum(2), target_type = contr.sum(2))) %>%
  tidy() %>%
  filter(effect == "fixed")

```

The previous section suggests that children have difficulty recognizing the referential ambiguity of a canonical label like "dog" in a context where there are multiple dogs.
In other words, children's production may be constrained by limitations in their *pragmatic reasoning*. 
But why would insensitivity to pragmatic context be limited to familiar objects?
One possibility is that children have a less stable prior over possible names for the novel tangram images due to lower 'codability' or 'nameability' [@hupet1991effects; @zettersten2020].
Their variability across contexts could be driven less by pragmatic reasoning and more by "sampling variation" [@denison2013rational; @bonawitz2014probabilistic].

We test this hypothesis by examining the distribution of labels at the population level. 
First, we hypothesize that both adults and children will produce a fairly narrow, high-agreement range of labels for familiar objects, yielding highly concentrated distributions.
Second, we hypothesize that children will use a broader range of different labels for tangrams than adults, yielding a less concentrated distribution with less agreement among different children.
We considered several measures of concentration, but we focus primarily on the proportion of unique labels to total labels, which is simple and interpretable.^[In principle, the most appropriate metric of spread across labels would be the information theoretic quantity of entropy: $H(X) = \sum_i p_i \log p_i$. However, our empirical distributions are highly sparse, with many labels appearing only once. Our estimates of entropy are therefore somewhat sensitive to the choice of statistical estimator (e.g. how much to regularize with pseudo-counts) while being less numerically interpretable. However, we find a similar qualitative effects of interest for reasonable choices, $b=$`r (var.out.entropy %>% filter(term == 'target_type1:age_group1'))$estimate`, $t$(`r (var.out.entropy %>% filter(term == 'target_type1:age_group1'))$df`)$=$ `r (var.out.entropy %>% filter(term == 'target_type1:age_group1'))$statistic`, $p =$ `r (var.out.entropy %>% filter(term == 'target_type1:age_group1'))$p.value` using the Schurmann-Grassberger estimator. Alternative metrics include "modal agreement" [@brodeur2010bank; @brodeur2014bank], the proportion of participants that produce the most common label, and Simpson's diversity index [@simpson1949measurement; @majid2014odors; @majid2018differential], which can be interpreted as the probability that two independently sampled labels will match.]
For example, suppose that from a pool of forty participants, twenty said 'bird', ten said 'dancer', and the other ten said something unique. 
Then we would have $p_{uniq} = 12/40 = .3$.
Meanwhile, if all forty participants chose different labels, we would have $p_{uniq} = 40/40 = 1$; and at the other extreme, if all forty participants chose the same label, we would have $p_{uniq} = 1/40 = 0.025$.

Because population agreement metrics necessarily aggregate over individual participants for each target, we construct our mixed-effects regression model at the item level. 
Given our findings of differential context-sensitivity in the previous section, we limit this analysis to the 'far' condition where the distribution of adult and children labels are more comparable (aggregating across close and far yields qualitatively similar results). 
We predict agreement as a function of age group (adult vs. child) and target type (familiar vs. tangram), including random intercepts at the target level. 
First, we observe a main effect of target type, with less agreement on tangram labels for all participants, $b = `r (var.out %>% filter(term == 'target_type1'))$estimate`$, $t(`r (var.out %>% filter(term == 'target_type1'))$df`) = `r (var.out %>% filter(term == 'target_type1'))$statistic`$, $p < 0.001$. This is in line with our hypothesis that agreement would be higher for familiar objects with commonly-known canonical labels.
Importantly, however, we also find a significant interaction with age group, $b = `r (var.out %>% filter(term == 'target_type1:age_group1'))$estimate`$, $t(`r (var.out %>% filter(term == 'target_type1:age_group1'))$df`) = `r (var.out %>% filter(term == 'target_type1:age_group1'))$statistic`$, $p < 0.001$. 
The labels produced by different children in our sample agree with one another about as much as adults' agree for familiar targets ($m = `r var.out.means$familiar_kid`$ for children and $m = `r var.out.means$familiar_adult`$ for adults). 
However, children as a group produce a much more variable set of labels for novel tangrams than adults do ($m = `r var.out.means$tangram_kid`$ for children and $m = `r var.out.means$tangram_adult`$ for adults).


```{r overlap-helpers, eval = FALSE}
# This analysis asks how much *children* overlapped with *adults*
# i.e. it's possible that kids agree with other kids and adults agree with other adults,
# but kids might be drawing from a completely different pool of utterances than adults.

# ps <- d.prod.clean %>%
#   distinct(wID)
# 
# first_half <- d.prod.clean %>%
#   filter(wID %in% (slice(ps, 1:(n()/2)) %>% pull(wID))) %>%
#   count(target, clean_utt) %>%
#   complete(target, clean_utt, 
#            fill = list(n = 0)) %>%
#   get_average_overlap()
# 
# second_half <- d %>%
#   filter(wID %in% (slice(ps, ((n()/2)+1):(n())) %>% pull(wID))) %>%
#   count(target, lemmatized_utt) %>%
#   complete(target, lemmatized_utt, 
#            fill = list(n = 0)) %>%
#   get_average_overlap()
# 
# # correlation b/w split data
# first_half %>%
#   rename(first_half = overlap) %>%
#   left_join(second_half, by = c("target_id", "comparison_id", "n")) %>%
#   summarise(cor = cor(first_half, overlap, method = "spearman")) %>%
#   summarise(cor = mean(cor, na.rm = T))
# 
# overall <- d %>%
#   #filter(!(target %in% c("R1.jpg", "O1.jpg"))) %>%
#   #mutate(target = as.factor(target)) %>%
#   count(target, lemmatized_utt) %>%
#   complete(target, lemmatized_utt, 
#            fill = list(n = 0)) %>%
#   get_average_overlap()
```

### Children's understanding of referential ambiguity predicts production

Children are generally less sensitive to referential context than adults, but what contributes to these differences? 
Given that children are more likely to produce the same label across contexts for familiar objects, it is unlikely that vocabulary knowledge accounts for most of the difference between children and adults. 
One factor that may influence children's performance is the recognition of referential ambiguity. 
That is, children who understand that ambiguous referring expressions are not sufficiently informative for a partner should show more context sensitivity in our production task.

To assess that children understand the goal of the task, and to aid in our interpretation of any potentially ambiguous responses that children give in earlier trials, we included a manipulation check condition for children. 
We first report children's responses to all three check trial types. 
Then, we turn to analyze how age and responses to ambiguous check trials (e.g., "dog" to distinguish between a husky and a dalmatian) predicts children's context sensitivity on the production task.

```{r check-trials}

check_trial_data <- d.prod.clean.manual %>%
  filter(competitor_type == "check")

ground_truth_responses <- check_trial_data %>%
  distinct(utterance) %>%
  mutate(unambiguous = utterance %in% c("polar bear", "red flower", "m&ms"),
         wrong = utterance %in% c("cup", "table", "bear"),
         ambiguous = utterance %in% c("dog", "bird", "shirt"))

joined_check_trial_data <- check_trial_data %>%
  left_join(ground_truth_responses, by = "utterance") %>%
  mutate(correct = (unambiguous & button_pressed == 0) | 
           (!unambiguous & button_pressed == 1))
  
subj_check_trial_data <- joined_check_trial_data %>%  
  group_by(age, id) %>%
  summarise(correct = mean(correct))

type_check_trial_data <- joined_check_trial_data %>%
  mutate(check_type = case_when(
    unambiguous ~ "unambiguous",
    wrong ~ "wrong",
    T ~ "ambiguous")) %>%
  group_by(check_type, age, id) %>%
  summarise(correct = mean(correct))

ggplot(type_check_trial_data, aes(x = age, y = correct, color = check_type)) +
  facet_wrap(~ check_type) +
  geom_hline(aes(yintercept = .5), linetype = "dashed") +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
  geom_jitter(width = 0.05, height = 0.05)

age_model <- type_check_trial_data %>%
  lmer(correct ~ age*check_type + (1|id), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")

check_means <- type_check_trial_data %>%
  ungroup() %>%
  group_by(check_type) %>%
  tidyboot_mean(correct)
  
```


```{r eval = FALSE}
type_check_trial_data %>%
  group_by(age, check_type) %>%
  tidyboot_mean(correct, nboot = 100) %>%
  ggplot(aes(x = age, y = empirical_stat, color = check_type)) +
    facet_wrap(~ check_type) +
    geom_hline(aes(yintercept = .5), linetype = "dashed") +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    #geom_bin_2d(binwidth = c(1, 0.25))
    geom_smooth(method = 'lm', formula = y ~ poly(x, 2), se = F) +
    geom_point(aes(size = n))

```

```{r plot-check-data, eval = FALSE}
ggplot(subj_check_trial_data, aes(x = age, y = correct)) + 
  geom_jitter() + 
  geom_hline(aes(yintercept = .5), linetype = "dashed")

subj_check_trial_data %>%
  ungroup() %>%
  tidyboot_mean(correct)
```

For all check trials, children were asked to respond "Yes" or "No" as to whether a provided label uniquely identifies one picture out of two. If children are sensitive to informativity of referential expressions, they should respond "Yes" to Close trials with unambiguous labels (e.g., "red flower" to distinguish between a rose and a lily), "No" to Far trials where the provided labels were unambiguous but incorrect (e.g., "table" to distinguish between a pug and a rabbit), and "No" to Close trials with ambiguous expressions (e.g., "dog" to distinguish between a husky and a dalmatian).

We found that children responded as expected to the unambiguous trials. That is, they responded "No" to the incorrect trial ($m=$ `r check_means%>%filter(check_type == "wrong")%>% pull(mean)`), and "Yes" to the unambiguous trial ($m=$ `r check_means%>%filter(check_type == "unambiguous")%>%pull(mean)`). These findings suggest that children, regardless of age, understood that that goal of the task was to describe a target picture (and not, for instance, to simply say anything). We also found an overall effect of age, where children's accuracy on the check trials increased with age ($b= `r age_model%>%filter(term == "age")%>%pull(estimate)`$, $t(`r age_model%>%filter(term == "age")%>%pull(df)`) = `r age_model%>%filter(term == "age")%>%pull(statistic)`$, $p < 0.001$). Further, we found an effect of trial type, such that children were more likely to be correct on unambiguous ($b= `r age_model%>%filter(term == "check_typeunambiguous")%>%pull(estimate)`$, $t(`r age_model%>%filter(term == "check_typeunambiguous")%>%pull(df)`) = `r age_model%>%filter(term == "check_typeunambiguous")%>%pull(statistic)`$, $p<0.001$) and incorrect trials ($b= `r age_model%>%filter(term == "check_typewrong")%>%pull(estimate)`$, $t(`r age_model%>%filter(term == "check_typewrong")%>%pull(df)`) = `r age_model%>%filter(term == "check_typewrong")%>%pull(statistic)`$, $p<0.001$) than on ambiguous trials ($m =$ `r check_means%>%filter(check_type == "ambiguous")%>%pull(mean)`).

```{r}
kid_data <- indiv.measures %>%
  filter(age_group=="kid") 

#predicting context sensitivity by overall manipulation check performance
kid_model_data <- kid_data %>%
  group_by(id, target_type) %>%
  tidyboot_mean(close_far.distance) %>%
  left_join(subj_check_trial_data, by = "id")

kid_model <- kid_model_data %>%
  lm(empirical_stat ~ target_type*correct, data = .) %>%
  tidy() 

ggplot(kid_model_data, aes(x = correct, y = empirical_stat)) +
         geom_jitter() +
         geom_smooth(method = "lm") +
         facet_wrap(~target_type)

#only ambiguous manipulation check trials

ambiguous_check_data <- type_check_trial_data %>%
  filter(check_type == "ambiguous")

kid_ambiguous_model_data <- kid_data %>%
  group_by(id, target_type) %>%
  tidyboot_mean(close_far.distance) %>%
  left_join(ambiguous_check_data, by = "id")

kid_ambiguous_model_data$target_type <- relevel(factor(kid_ambiguous_model_data$target_type), ref = "tangram")

kid_ambiguous_model <- kid_ambiguous_model_data %>%
  lmer(empirical_stat ~ target_type*correct + 
         (1|id) + (1|age), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")

kid_age_model <- kid_ambiguous_model_data %>%
  lmer(empirical_stat ~ age*target_type + (1|id), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")

#predicting context sensitivity by age

kid_age_data <- kid_data %>%
  left_join(subj_check_trial_data, by = "id") %>%
  group_by(age, target_type) %>%
  tidyboot_mean(close_far.distance)

kid_age_model <- kid_age_data %>%
  lm(empirical_stat ~ age*target_type, data = .) %>%
  tidy()

ggplot(kid_age_data, aes(x = age, y = empirical_stat)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  facet_wrap(~target_type)
```

```{r e3-check, fig.pos = "t", fig.align = "center", fig.width = 10, fig.cap = "\\label{fig:e3_check} Children's performance on ambiguous manipulation trials predicted their context sensitivity for familiar objects, but not for tangrams."}

ggplot(kid_ambiguous_model_data, aes(x = correct, y = empirical_stat)) +
         geom_point() +
         geom_smooth(method = "lm") +
         facet_wrap(~target_type)

```
To assess whether recognition of referential ambiguity correlated with performance on the production task, we focused specifically on children's performance on the ambiguous check trials. On these trials, children acted as the listener, and determined whether a give description was sufficient for distinguishing a target from its distractor. We predict that children who perform better on these ambiguous trials would also produce more informative descriptions during our task.

We fit a mixed-effects model predicting children's context sensitivity, as measured by 'exact match' overlap, from their performance on the ambiguous trials and target type (familiar vs. tangram), with random effects of participant and age. We found a significant interaction between accuracy on the ambiguous trials and target type, such that children who were more accurate on the ambiguous check trials were more likely to modulate their descriptions based on context for familiar items ($b = `r kid_ambiguous_model%>%filter(term == "target_typefamiliar:correct")%>%pull(estimate)`,~t(`r kid_ambiguous_model%>%filter(term == "target_typefamiliar:correct")%>%pull(df)`) = `r kid_ambiguous_model%>%filter(term == "target_typefamiliar:correct")%>%pull(statistic)`, p = `r kid_ambiguous_model%>%filter(term == "target_typefamiliar:correct")%>%pull(p.value)`$). In other words, children who were more sensitive to referential ambiguity were less likely to produce the same descriptions for familiar items across close and far contexts (Figure \ref{fig:e3_check}). 

## Discussion

In Experiment 3, we asked whether difficulty in *production* may explain children's struggle to form referential pacts with peers. Comparing adult and child responses on our production task revealed that children were not sensitive to referential contexts, providing descriptions that did not always distinguish the intended referent from its distractor (e.g., saying "table" when there was a dining table and a picnic table). Importantly, this effect was found in trials containing familiar items that children should have labels for, so our results were not due to simple vocabulary constraints. 

Why might children struggle to modulate their descriptions for familiar objects? One possibility is that the basic level category label of a familiar object (e.g., "table") is too salient, and children have difficulty suppressing this label in favor or a more informative description. While we cannot infer whether children were considering alternative descriptions during our task, we did find that children, compared to adults, produced less variable descriptions for familiar objects. Further studies probing the alternatives that children consider, or test the salience of various object labels, could shed light onto the potential mechanisms underly children's ability to modulate descriptions based on context.

Another account for why children struggle to modify their descriptions is that they are not sensitive to referential ambiguity. Indeed, prior work shows that children may not know which expressions are more informative [e.g., @beal1982; @robinson1977]. Our experiment provides evidence for a direct link between *recognizing* referential ambiguity and *producing* informative expressions based on context. Children who recognized that ambiguous expressions are not helpful for reference were more likely to show context sensitivity in their own descriptions of familiar objects.

Results from our production task revealed that children's difficulty in forming conceptual pacts with peers may arise from their struggle to recognize referential ambiguity which, in turn, can lead to under-informative expressions. Children's success in Experiment 1 may thus lie in the supportive context that parents provided. Specifically, behaviors such as prompting or asking clarification questions could provide children with feedback that their expressions are not sufficient for distinguishing a target from its distractor. 
