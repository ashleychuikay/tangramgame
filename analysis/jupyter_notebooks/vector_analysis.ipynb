{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from nlp_utils import get_feats, lemmatize_doc\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine semantic embeddings\n",
    "We'd like to pull out bag of words embeddings from NPs in each utterance in the cued dataset and cluster them for each tangram; expect to see different pairs in different parts of the space (i.e. to compute a d' for an 'idiosyncracy' or 'multiple equilibria' result) and also different utterances from single games closer together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-84be5ca31272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m d_raw = (pd.read_csv('../data/deidentified/combined_clean.csv', encoding='latin-1')\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utterance != \"x\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          .reset_index())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "d_raw = (pd.read_csv('../data/deidentified/combined_clean.csv', encoding='latin-1')\n",
    "         .query('utterance != \"x\"')\n",
    "         .reset_index())\n",
    "# Remove null utterances\n",
    "d_raw = d_raw[pd.notnull(d_raw.utterance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate director & matcher utterances\n",
    "d_raw.groupby(['subid','trial', 'target', 'age', 'rep_num', 'experiment', 'director'])['utterance'].agg(utterance=' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse with spacy\n",
    "d_raw['text'] = [nlp(text) for text in d_raw['utterance']]\n",
    "d_raw['contentful'] = [[t.lemma_ for t in text \n",
    "                        if t.is_alpha and \n",
    "                        t.pos_ not in ['PRON', 'DET', 'CCONJ', 'ADP', 'AUX', 'PUNCT']] \n",
    "                       for text in d_raw['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_embedding = np.full((1,300), np.nan)\n",
    "def get_feats(d_in, nlp, scramble = False) :\n",
    "    # only look at director utterances\n",
    "    d = d_in.copy()\n",
    "\n",
    "    # initialize feature vector\n",
    "    raw_avg_feats = np.array([]).reshape(0, 300)\n",
    "\n",
    "    if scramble :\n",
    "        d = scramble_words(d)\n",
    "        \n",
    "    for i, row in d.iterrows() :\n",
    "        local_embedding = np.array([]).reshape(0, 300)\n",
    "        for token in row['contentful'] :\n",
    "            if nlp(token).has_vector and sum(nlp(token).vector) != 0:\n",
    "                local_embedding = np.vstack((local_embedding, nlp(token).vector))\n",
    "            else :\n",
    "                print(i, '/', d.shape[0])\n",
    "                print(row['utterance'])\n",
    "                print(row['contentful'])\n",
    "                print('no vector available:', nlp(token))\n",
    "\n",
    "        # average them together, handling empty lists\n",
    "        if row['contentful'] :\n",
    "            raw_avg_embedding = np.nanmean(local_embedding, axis = 0) \n",
    "        else :\n",
    "            \n",
    "            raw_avg_embedding = null_embedding.copy()\n",
    "            row['is_null'] = True\n",
    "            \n",
    "        # add to overall list\n",
    "        raw_avg_feats = np.vstack((raw_avg_feats, raw_avg_embedding))\n",
    "    return d, raw_avg_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, raw_avg_feats = get_feats(d_raw, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(meta\n",
    " .drop(columns = [ 'utterance', 'contentful'])\n",
    " .to_csv('../data/deidentified/meta_tangrams_embeddings.csv'))\n",
    "np.save('../data/deidentified/feats_tangrams_embeddings_rawavg.npy', raw_avg_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at tsne visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2)\n",
    "big_pca = PCA(n_components = 40)\n",
    "viz_pca = PCA(n_components = 2)\n",
    "mds = MDS(n_components=2)\n",
    "embedding_viz = pd.DataFrame(\n",
    "    columns = ['subid', 'target', 'trial', 'rep_num', 'x_tsne', 'y_tsne', 'x_mds', 'y_mds', 'feats_type']\n",
    ")\n",
    "\n",
    "for name, group in meta.reset_index(drop=True).groupby('target') :\n",
    "    tangram_inds = np.array(group.index)\n",
    "    feats = raw_avg_feats\n",
    "    relevant_feats = feats[tangram_inds]\n",
    "    \n",
    "    # You can't run tsne with NANs, so we have to take them out and then add them back in...\n",
    "    nan_rows = [i for i in range(relevant_feats.shape[0]) if pd.isna(relevant_feats[i,0])]\n",
    "    nan_insert_rows = [k - lag for (lag, k) in enumerate(nan_rows)]\n",
    "    X = np.ma.masked_invalid(relevant_feats)\n",
    "    tsne_out = tsne.fit_transform(big_pca.fit_transform(np.ma.compress_rows(X)))\n",
    "    tsne_out = np.insert(tsne_out, nan_insert_rows, np.nan, axis=0)\n",
    "    X_tsne = pd.DataFrame(tsne_out, columns = ['x_tsne', 'y_tsne'], index=tangram_inds) \n",
    "    embedding_viz = embedding_viz.append(pd.concat([group, X_tsne], axis = 1), \n",
    "                                         ignore_index=True, sort=False)\n",
    "embedding_viz.drop(columns=['text', 'contentful']).to_csv('../data/deidentified/tsne_embeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
